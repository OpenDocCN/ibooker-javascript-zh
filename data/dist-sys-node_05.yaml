- en: Chapter 4\. Observability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。可观察性
- en: This chapter is dedicated to observing Node.js services that run on remote machines.
    Locally, tools like the debugger or `console.log()` make this a straightforward
    process. However, once a service is running in a faraway land, you’ll need to
    reach for a different set of tools.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章专注于观察在远程机器上运行的Node.js服务。在本地，像调试器或`console.log()`这样的工具使得这个过程非常直接。然而，一旦服务在远方运行，你就需要借助不同的工具集。
- en: When debugging locally, you’re usually concerned with a single request. You
    might ask yourself, “When I pass this value into a request, why do I get that
    value in the response?” By logging the inner workings of a function, you gain
    insight into why a function behaved in an unanticipated way. This chapter looks
    at technologies useful for debugging individual requests as well. [“Logging with
    ELK”](#ch_monitoring_sec_log) looks at log generation, which is a way to keep
    track of information on a per-request basis, much like you might print with `console.log()`.
    Later, [“Distributed Request Tracing with Zipkin”](#ch_monitoring_sec_trace) looks
    at a tool for tracking requests as they’re passed around, associating related
    logs generated by different services.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地调试时，通常关注的是单个请求。你可能会问自己：“当我将这个值传递给请求时，为什么我在响应中得到那个值？”通过记录函数的内部工作原理，你可以深入了解函数为何表现出意外的方式。本章还探讨了对调试单个请求有用的技术。[“使用ELK进行日志记录”](#ch_monitoring_sec_log)介绍了日志生成，这是一种类似于使用`console.log()`打印信息的方式。稍后，[“使用Zipkin进行分布式请求跟踪”](#ch_monitoring_sec_trace)介绍了一种跟踪请求的工具，它会关联不同服务生成的相关日志。
- en: You often need insight into situations that wouldn’t normally be considered
    a hard bug when dealing with production traffic. For example, you might have to
    ask, “Why are HTTP requests 100ms slower for users created before April 2020?”
    Such timing might not be worrying with a single request, but when such metrics
    are considered in aggregate over many requests, you’re able to spot trends of
    negative performance. [“Metrics with Graphite, StatsD, and Grafana”](#ch_monitoring_sec_metrics)
    covers this in more detail.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理生产流量时，通常需要深入了解通常不被视为严重错误的情况。例如，你可能会问：“为什么在2020年4月之前创建的用户的HTTP请求要慢100毫秒？”单个请求的这种时序可能并不令人担忧，但当考虑到多个请求的总体度量时，你可以发现性能下降的趋势。[“使用Graphite、StatsD和Grafana进行度量”](#ch_monitoring_sec_metrics)更详细地介绍了这一点。
- en: These tools mostly display information passively in a dashboard of some sort,
    which an engineer can later consult to determine the source of a problem. [“Alerting
    with Cabot”](#ch_monitoring_sec_alert) covers how to send a warning to a developer
    when an application’s performance dips below a certain threshold, thus allowing
    the engineer to prevent an outage before it happens.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具大多数以某种仪表板 passively 显示信息，工程师稍后可以查阅以确定问题的根源。[“使用Cabot进行警报”](#ch_monitoring_sec_alert)介绍了如何在应用程序性能低于某个阈值时向开发人员发送警告，从而使工程师能够在问题发生之前防止停机。
- en: So far these concepts have been reactive, where a developer must look at data
    captured from an application. Other times it’s necessary to be more proactive.
    [“Health Checks”](#ch_monitoring_sec_health) covers how an application can determine
    if it’s healthy and able to serve requests or if it’s unhealthy and deserves to
    be terminated.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这些概念都是被动的，开发人员必须查看从应用程序捕获的数据。有时需要更主动的方法。[“健康检查”](#ch_monitoring_sec_health)介绍了应用程序如何确定其是否健康并能够提供请求，或者是否不健康并应该被终止。
- en: Environments
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: '*Environments* are a concept for differentiating running instances of an application,
    as well as databases, from each other. They’re important for various reasons,
    including choosing which instances to route traffic to, keeping metrics and logs
    separate (which is particularly important in this chapter), segregating services
    for security, and gaining confidence that a checkout of application code is going
    to be stable in one environment before it is deployed to production.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*环境*是区分应用程序实例和数据库之间运行的概念的概念。它们的重要性有多种原因，包括选择要将流量路由到哪些实例，保持度量和日志的分离（这在本章中尤为重要），为安全性隔离服务，以及在将应用程序代码检出到生产环境之前获得其稳定性的信心。'
- en: Environments should remain segregated from one another. If you control your
    own hardware, this could mean running different environments on different physical
    servers. If you’re deploying your application to the cloud, this more likely means
    setting up different VPCs (Virtual Private Clouds)—a concept supported by both
    AWS and GCP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 各个环境应该相互隔离。如果你控制自己的硬件，这可能意味着在不同的物理服务器上运行不同的环境。如果你将应用程序部署到云上，这更可能意味着设置不同的VPCs（虚拟私有云）——这是AWS和GCP都支持的概念。
- en: At an absolute minimum, any application will need at least a single *production*
    environment. This is the environment responsible for handling requests made by
    public users. However, you’re going to want a few more environments than that,
    especially as your application grows in complexity.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，任何应用程序都需要至少一个*生产*环境。这个环境负责处理公共用户的请求。然而，你可能需要比这更多的环境，特别是当你的应用程序在复杂性上增长时。
- en: 'As a convention, Node.js applications generally use the `NODE_ENV` environment
    variable to specify which environment an instance is running in. This value can
    be set in different ways. For testing, it can be set manually, like with the following
    example, but for production use, whatever tool you use for deploying will abstract
    this process away:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个惯例，Node.js应用程序通常使用`NODE_ENV`环境变量来指定实例运行的环境。可以通过不同的方式设置这个值。对于测试，可以像以下示例一样手动设置，但对于生产使用，无论你使用哪种部署工具，都会抽象化这个过程：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Philosophies for choosing *what* code to deploy to different environments, which
    branching and merging strategies to use, and even which VCS (version control system)
    to choose are outside the scope of this book. But, ultimately, a particular snapshot
    of the codebase is chosen to be deployed to a particular environment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 选择*部署*到不同环境的代码的哲学，使用的分支和合并策略，甚至选择哪种VCS（版本控制系统），这些都超出了本书的范围。但是，最终会选择代码库的特定快照来部署到特定环境。
- en: 'Choosing *which* environments to support is also important, and also outside
    the scope of this book. Usually companies will have, at a minimum, the following
    environments:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 选择支持的*环境*也很重要，也超出了本书的范围。通常公司至少会有以下环境：
- en: Development
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 开发环境
- en: Used for local development. Perhaps other services know to ignore messages associated
    with this environment. Doesn’t need some of the backing stores required by production;
    for example, logs might be written to *stdout* instead of being transmitted to
    a collector.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用于本地开发。其他服务可能知道忽略与该环境相关的消息。不需要生产环境所需的一些后备存储；例如，日志可能会写入*stdout*，而不是传输到收集器。
- en: Staging
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 暂存环境
- en: Represents an exact copy of the *production* environment, such as machine specs
    and operating system versions. Perhaps an anonymized database snapshot from production
    is copied to a *staging* database via a nightly cron job.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表示*生产*环境的精确副本，如机器规格和操作系统版本。可能会通过每晚的cron作业将生产环境的匿名数据库快照复制到*暂存*数据库。
- en: Production
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境
- en: Where the real production traffic is processed. There may be more service instances
    here than in *staging*; for example, maybe *staging* runs two application instances
    (always run more than one) but *production* runs eight.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 处理真实生产流量的地方。可能比*暂存*运行更多服务实例；例如，也许*暂存*运行两个应用程序实例（始终运行多于一个），但*生产*运行八个。
- en: The environment string must remain consistent across all applications, both
    those written using Node.js and those on other platforms. This consistency will
    prevent many headaches. If one team refers to an environment as *staging* and
    the other as *preprod*, querying logs for related messages then becomes an error-prone
    process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 环境字符串必须在所有应用程序中保持一致，无论是使用Node.js编写的应用程序还是其他平台的应用程序。这种一致性将避免许多头痛。如果一个团队称环境为*暂存*，而另一个称其为*预发布*，那么查询相关消息的日志就会变得容易出错。
- en: The environment value shouldn’t necessarily be used for configuration—for example,
    having a lookup map where environment name is associated with a hostname for a
    database. Ideally, any dynamic configuration should be provided via environment
    variables. Instead, the environment value is mostly used for things related to
    observability. For example, log messages should have the environment attached
    in order to help associate any logs with the given environment, which is especially
    important if a logging service does get shared across environments. [“Application
    Configuration”](ch10.html#ch_security_sec_config) takes a deeper look at configuration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 环境值不一定用于配置，例如，有一个查找映射，其中环境名称与数据库的主机名相关联。理想情况下，任何动态配置应通过环境变量提供。相反，环境值主要用于与可观察性相关的事物。例如，日志消息应该附加环境，以帮助将任何日志与给定的环境关联起来，如果一个日志服务确实跨环境共享，这一点尤为重要。[“应用程序配置”](ch10.html#ch_security_sec_config)深入探讨了配置。
- en: Logging with ELK
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录日志使用 ELK
- en: '*ELK*, or more specifically, *the ELK stack*, is a reference to *Elasticsearch*,
    *Logstash*, and *Kibana*, three open source tools built by [Elastic](https://elastic.co).
    When combined, these powerful tools are often the platform of choice for collecting
    logs on-prem. Individually, each of these tools serves a different purpose:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*ELK*，或更具体地说，*ELK 栈*，是对由[Elastic](https://elastic.co)构建的三个开源工具 *Elasticsearch*、*Logstash*
    和 *Kibana* 的引用。当这些强大的工具组合在一起时，它们通常是在本地收集日志的首选平台。单独地，每个工具都有不同的用途：'
- en: Elasticsearch
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch
- en: A database with a powerful query syntax, supporting features like natural text
    searching. It is useful in many more situations than what are covered in this
    book and is worth considering if you ever need to build a search engine. It exposes
    an HTTP API and has a default port of `:9200`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有强大查询语法的数据库，支持自然文本搜索等功能。在这本书涵盖的情况之外，它在许多其他情况下也很有用，如果您需要构建搜索引擎，值得考虑。它通过 HTTP
    API 暴露，并具有默认端口`:9200`。
- en: Logstash
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash
- en: A service for ingesting and transforming logs from multiple sources. You’ll
    create an interface so that it can ingest logs via User Diagram Protocol (UDP).
    It doesn’t have a default port, so we’ll just use `:7777`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 用于从多个来源摄取和转换日志的服务。您将创建一个接口，以便它可以通过用户图协议（UDP）摄取日志。它没有默认端口，因此我们将只使用`:7777`。
- en: Kibana
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kibana
- en: A web service for building dashboards that visualize data stored in Elasticsearch.
    It exposes an HTTP web service over the port `:5601`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于构建可视化存储在 Elasticsearch 中的数据的仪表板的 Web 服务。它通过端口`:5601`暴露了一个 HTTP Web 服务。
- en: '[Figure 4-1](#fig_elk) diagrams these services and their relationships, as
    well as how they’re encapsulated using Docker in the upcoming examples.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-1](#fig_elk) 描述了这些服务及其关系，以及它们如何在即将的示例中使用 Docker 封装。'
- en: '![Administrator uses Kibana, Kibana talks to Elasticsearch, Applications logs
    to Logstash, and Logstash stores in Elasticsearch](assets/dsnj_0401.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![管理员使用 Kibana，Kibana 与 Elasticsearch 交互，应用程序日志到 Logstash，Logstash 存储在 Elasticsearch
    中](assets/dsnj_0401.png)'
- en: Figure 4-1\. The ELK stack
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. ELK 栈
- en: Your application is expected to transmit well-formed JSON logs, typically an
    object that’s one or two levels deep. These objects contain generic metadata about
    the message being logged, such as timestamp and host and IP address, as well as
    information specific to the message itself, such as level/severity, environment,
    and a human-readable message. There are multiple ways to configure ELK to receive
    such messages, such as writing logs to a file and using Elastic’s Filebeat tool
    to collect them. The approach used in this section will configure Logstash to
    listen for incoming UDP messages.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 预计您的应用程序将传输格式良好的 JSON 日志，通常是一个或两个级别深的对象。这些对象包含有关所记录消息的通用元数据，例如时间戳、主机和 IP 地址，以及消息本身的特定信息，例如级别/严重性、环境和可读消息。有多种配置
    ELK 来接收这些消息的方法，例如将日志写入文件并使用 Elastic 的 Filebeat 工具来收集它们。本节中使用的方法将配置 Logstash 监听传入的
    UDP 消息。
- en: Running ELK via Docker
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Docker 运行 ELK
- en: In order to get your hands dirty, you’re going to run a single Docker container
    containing all three services. (Be sure to have Docker installed—see [Appendix B](app02.html#appendix_install_docker)
    for more information.) These examples won’t enable disk persistence. Within a
    larger organization, each of these services would perform better when installed
    on dedicated machines, and of course, persistence is vital.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了亲自动手，您将运行一个包含所有三个服务的单个 Docker 容器。 （请确保已安装 Docker——有关更多信息，请参阅 [Appendix B](app02.html#appendix_install_docker)。）这些示例不会启用磁盘持久性。在更大的组织中，每个这些服务在专用机器上安装时都会表现更好，并且当然，持久性是至关重要的。
- en: In order to configure Logstash to listen for UDP messages, a configuration file
    must first be created. The content for this file is available in [Example 4-1](#ex_elk_udp)
    and can be placed in a new directory at *misc/elk/udp.conf*. Once the file is
    created, you’ll make it available to the Logstash service running inside of the
    Docker container. This is done by using the `-v` volume flag, which allows a local
    filesystem path to be mounted inside of the container’s filesystem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了配置 Logstash 监听 UDP 消息，首先必须创建一个配置文件。此文件的内容可在 [Example 4-1](#ex_elk_udp) 中找到，并可以放置在
    *misc/elk/udp.conf* 的新目录中。创建文件后，您将通过使用 `-v` 卷标志将其可用于运行在 Docker 容器内部的 Logstash
    服务。
- en: Example 4-1\. *misc/elk/udp.conf*
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-1\. *misc/elk/udp.conf*
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For brevity’s sake, these examples use UDP for sending messages. This approach
    doesn’t come with the same features as others, such as delivery guarantees or
    back pressure support, but it does come with reduced overhead for the application.
    Be sure to research the best tool for your use-case.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，这些示例使用 UDP 发送消息。这种方法不具备其他方法的一些特性，比如传递保证或反压力支持，但它减少了应用程序的开销。请务必为您的用例研究最佳工具。
- en: Once the file has been created you’re ready to run the container using the commands
    in [Example 4-2](#ex_elk_docker). If you’re running Docker on a system-based Linux
    machine, you’ll need to run the `sysctl` command before the container will properly
    run, and you may omit the `-e` flag if you want. Otherwise, if you’re running
    Docker on a macOS machine, skip the `sysctl` flag.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 创建文件后，您可以使用 [Example 4-2](#ex_elk_docker) 中的命令运行容器。如果在基于 Linux 的系统上运行 Docker，则需要在容器正确运行之前运行
    `sysctl` 命令，并且如果需要，可以省略 `-e` 标志。否则，如果在 macOS 上运行 Docker，则跳过 `sysctl` 标志。
- en: Example 4-2\. Running ELK within Docker
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-2\. 在 Docker 中运行 ELK
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This command downloads files from Dockerhub and configures the service and may
    take a few minutes to run. Once your console calms down a bit, visit *http://localhost:5601*
    in your browser. If you see a successful message, then the service is now ready
    to receive messages.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令从 Dockerhub 下载文件并配置服务，可能需要几分钟才能运行。一旦您的控制台稍微安静下来，请在浏览器中访问 *http://localhost:5601*。如果看到成功消息，则服务现在已准备好接收消息。
- en: Transmitting Logs from Node.js
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Node.js 传输日志
- en: For this example, you’re going to again start by modifying an existing application.
    Copy the *web-api/consumer-http-basic.js* file created in [Example 1-7](ch01.html#ex_consumer)
    to *web-api/consumer-http-logs.js* as a starting point. Next, modify the file
    to look like the code in [Example 4-3](#ex_consumer_logs).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，您将再次开始修改一个现有的应用程序。将 [Example 1-7](ch01.html#ex_consumer) 中创建的 *web-api/consumer-http-basic.js*
    文件复制到 *web-api/consumer-http-logs.js* 作为起点。接下来，修改文件使其看起来像 [Example 4-3](#ex_consumer_logs)
    中的代码。
- en: Example 4-3\. *web-api/consumer-http-logs.js*
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-3\. *web-api/consumer-http-logs.js*
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_observability_CO1-1)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO1-1)'
- en: The new *logstash.js* file is now being loaded.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 新的 *logstash.js* 文件现在正在加载。
- en: '[![2](assets/2.png)](#co_observability_CO1-2)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO1-2)'
- en: The `middie` package allows Fastify to use generic middleware.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`middie` 包允许 Fastify 使用通用中间件。'
- en: '[![3](assets/3.png)](#co_observability_CO1-3)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO1-3)'
- en: A middleware to log incoming requests.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于记录传入请求的中间件。
- en: '[![4](assets/4.png)](#co_observability_CO1-4)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_observability_CO1-4)'
- en: A call to the logger that passes in request data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 传递请求数据的记录器调用。
- en: '[![5](assets/5.png)](#co_observability_CO1-5)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_observability_CO1-5)'
- en: A generic middleware for logging errors.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 用于记录错误的通用中间件。
- en: '[![6](assets/6.png)](#co_observability_CO1-6)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_observability_CO1-6)'
- en: Information about outbound requests is logged.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 记录有关出站请求的信息。
- en: '[![7](assets/7.png)](#co_observability_CO1-7)'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_observability_CO1-7)'
- en: Information about server starts is also logged.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 记录有关服务器启动的信息。
- en: This file logs some key pieces of information. The first thing logged is when
    the server starts. The second set of information is by way of a generic middleware
    handler. It logs data about any incoming request, including the path, the method,
    the IP address, and the user agent. This is similar to the access log for a traditional
    web server. Finally, the application tracks outbound requests to the *recipe-api*
    service.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件记录了一些关键信息。首先记录了服务器启动的时间。第二组信息通过一个通用的中间件处理程序。它记录了任何传入请求的数据，包括路径、方法、IP地址和用户代理。这类似于传统Web服务器的访问日志。最后，应用程序跟踪到*recipe-api*服务的出站请求。
- en: The contents of the *logstash.js* file might be more interesting. There are
    many libraries available on npm for transmitting logs to Logstash (`@log4js-node/logstashudp`
    is one such package). These libraries support a few methods for transmission,
    UDP included. Since the mechanism for sending logs is so simple, you’re going
    to reproduce a version from scratch. This is great for educational purposes, but
    a full-featured package from npm will make a better choice for a production application.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*logstash.js*文件的内容可能更有趣。npm上有许多库可用于将日志传输到Logstash（`@log4js-node/logstashudp`就是其中之一）。这些库支持几种传输方法，包括UDP。由于发送日志的机制如此简单，你将从头开始重现一个版本。这对教育目的非常有用，但对于生产应用程序，从npm中选择一个功能齐全的包会更好。'
- en: Create a new file called *web-api/logstash.js*. Unlike the other JavaScript
    files you’ve created so far, this one won’t be executed directly. Add the content
    from [Example 4-4](#ex_logstash) to this file.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为*web-api/logstash.js*的新文件。与迄今为止创建的其他JavaScript文件不同，这个文件不会直接执行。将[示例 4-4](#ex_logstash)中的内容添加到此文件。
- en: Example 4-4\. *web-api/logstash.js*
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. *web-api/logstash.js*
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_observability_CO2-1)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO2-1)'
- en: The built-in `dgram` module sends UDP messages.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的`dgram`模块发送UDP消息。
- en: '[![2](assets/2.png)](#co_observability_CO2-2)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO2-2)'
- en: The Logstash location is stored in `LOGSTASH`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Logstash位置存储在`LOGSTASH`中。
- en: '[![3](assets/3.png)](#co_observability_CO2-3)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO2-3)'
- en: Several fields are sent in the log message.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 日志消息中发送了多个字段。
- en: This basic Logstash module exports a function that application code calls to
    send a log. Many of the fields are automatically generated, like `@timestamp`,
    which represents the current time. The `app` field is the name of the running
    application and doesn’t need to be overridden by the caller. Other fields, like
    `severity` and `type`, are fields that the application is going to change all
    the time. The `fields` field represents additional key/value pairs the app might
    want to provide.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的Logstash模块导出一个应用代码调用的函数来发送日志。许多字段是自动生成的，比如`@timestamp`，它表示当前时间。`app`字段是正在运行的应用程序的名称，不需要被调用者覆盖。其他字段，如`severity`和`type`，是应用程序经常更改的字段。`fields`字段表示应用程序可能想提供的额外的键/值对。
- en: 'The `severity` field (often called the *log level* in other logging frameworks)
    refers to the importance of the log. Most logging packages support the following
    six values, originally made popular by the npm client: *error*, *warn*, *info*,
    *verbose*, *debug*, *silly*. It’s a common pattern with more “complete” logging
    packages to set a logging threshold via environment variable. For example, by
    setting the minimum severity to *verbose*, any messages with a lower severity
    (namely *debug* and *silly*) will get dropped. The overly simple *logstash.js*
    module doesn’t support this.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`severity`字段（在其他日志框架中通常称为*日志级别*）指的是日志的重要性。大多数日志包支持以下六个值，最初由npm客户端流行起来：*error*、*warn*、*info*、*verbose*、*debug*、*silly*。使用更“完整”的日志包通常通过环境变量设置日志阈值。例如，通过将最小严重性设置为*verbose*，任何具有更低严重性的消息（即*debug*和*silly*）将被丢弃。过于简单的*logstash.js*模块不支持此功能。'
- en: Once the payload has been constructed, it’s then converted into a JSON string
    and printed to the console to help tell what’s going on. Finally, the process
    attempts to transmit the message to the Logstash server (there is no way for the
    application to know if the message was delivered; this is the shortcoming of UDP).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦负载被构建，它就会被转换为JSON字符串并打印到控制台以帮助了解正在发生的事情。最后，进程尝试将消息传输到Logstash服务器（应用程序无法知道消息是否已传递；这是UDP的缺点）。
- en: With the two files created, it’s now time to test the application. Run the commands
    in [Example 4-5](#ex_logstash_run_app). This will start an instance of the new
    *web-api* service, an instance of the previous *recipe-api* service, and will
    also send a series of requests to the *web-api*. A log will be immediately sent
    once the *web-api* has been started, and two additional logs will be sent for
    each incoming HTTP request. Note that the `watch` commands continuously execute
    the command following on the same line and will need to be run in separate terminal
    windows.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已创建了两个文件，是时候测试应用程序了。运行 [示例 4-5](#ex_logstash_run_app) 中的命令。这将启动一个新的 *web-api*
    服务实例，以及前一个 *recipe-api* 服务的一个实例，并将一系列请求发送到 *web-api*。一旦 *web-api* 启动，将立即发送一个日志，并且每个传入的
    HTTP 请求将发送两个额外的日志。请注意，`watch` 命令将持续执行同一行后面的命令，并且需要在单独的终端窗口中运行。
- en: Example 4-5\. Running *web-api* and generating logs
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 运行 *web-api* 并生成日志
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Isn’t that exciting? Well, not quite yet. Now you’ll jump into Kibana and take
    a look at the logs being sent. Let the `watch` commands continue running in the
    background; they’ll keep the data fresh while you’re using Kibana.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是很激动人心吗？嗯，并不完全是。现在您将进入 Kibana 并查看发送的日志。让`watch`命令在后台继续运行；在您使用 Kibana 时，它们将保持数据的新鲜。
- en: Creating a Kibana Dashboard
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Kibana 仪表板
- en: Now that the application is sending data to Logstash and Logstash is storing
    the data in Elasticsearch, it’s time to open Kibana and explore this data. Open
    your browser and visit [*http://localhost:5601*](http://localhost:5601). At this
    point you should be greeted with the Kibana dashboard.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应用程序正在将数据发送到 Logstash，并且 Logstash 正在将数据存储在 Elasticsearch 中，是时候打开 Kibana 并探索这些数据了。打开您的浏览器并访问
    [*http://localhost:5601*](http://localhost:5601)。此时，您应该会看到 Kibana 仪表板的欢迎界面。
- en: Within the dashboard, click the last tab on the left, titled [Management](http://localhost:5601/app/kibana#/management).
    Next, locate the Kibana section of options and then click the Index Patterns option.
    Click Create index pattern. For Step 1, type in an Index pattern of `nodejs-*`.
    You should see a small Success! message below as Kibana correlates your query
    to a result. Click Next step. For Step 2, click the Time Filter drop-down menu
    and then click the `@timestamp` field. Finally, click Create index pattern. You’ve
    now created an index named `nodejs-*` that will allow you to query those values.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在仪表板中，点击左侧的最后一个选项卡，标题为 [管理](http://localhost:5601/app/kibana#/management)。接下来，找到
    Kibana 选项部分，然后点击索引模式选项。点击创建索引模式。对于步骤 1，输入一个索引模式为 `nodejs-*`。您应该会看到下方显示一个小的“成功！”消息，因为
    Kibana 正在将您的查询与结果相关联。点击下一步。对于步骤 2，点击时间过滤器下拉菜单，然后点击 `@timestamp` 字段。最后，点击创建索引模式。您现在已创建了一个名为
    `nodejs-*` 的索引，可以用来查询这些值。
- en: Click the second tab on the left, titled [Visualize](http://localhost:5601/app/kibana#/visualize).
    Next, click the Create new visualization button in the center of the screen. You’ll
    be given several different options for creating a visualization, including the
    ones shown in [Figure 4-2](#fig_kibana_visualizations), but for now just click
    the Vertical Bar graph option.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 点击左侧的第二个选项卡，标题为 [可视化](http://localhost:5601/app/kibana#/visualize)。接下来，点击屏幕中央的创建新可视化按钮。您将看到几种不同的选项来创建可视化，包括[图 4-2](#fig_kibana_visualizations)中显示的选项，但现在只需点击垂直条形图选项。
- en: '![Kibana Icons: Gauge, Goal, Heat Map, Horizontal Bar, Line, Maps, Markdown,
    Metric](assets/dsnj_0402.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![Kibana 图标：仪表，目标，热图，水平条形图，线条，地图，Markdown，度量](assets/dsnj_0402.png)'
- en: Figure 4-2\. Kibana visualizations
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. Kibana 可视化
- en: Select the `nodejs-*` index that you just created. Once that’s done, you’ll
    be taken to a new screen to fine-tune the visualization. The default graph isn’t
    too interesting; it’s a single bar showing a count of all logs matching the `nodejs-*`
    index. But not for long.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 选择刚创建的 `nodejs-*` 索引。完成后，您将被带到一个新的屏幕来微调可视化。默认的图表并不太有趣；它只显示一个条形图，显示与 `nodejs-*`
    索引匹配的所有日志的计数。但这不会持续太久。
- en: The goal now is to create a graph that displays the rate at which incoming requests
    are received by the *web-api* service. So, first add a few filters to narrow down
    the results to only contain applicable entries. Click the Add a Filter link near
    the upper-left corner of the screen. For the Field drop-down menu, enter the value
    `type`. For the Operator field, set it to `is`. For the Value field, enter the
    value `request-incoming` and then click Save. Next, click Add a Filter again and
    do the same thing, but this time set Field to `app`, then set Operator to `is`
    again, and set Value to `web-api`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的目标是创建一个图表，显示 *web-api* 服务接收传入请求的速率。首先添加一些过滤器，以缩小结果范围，只包含适用的条目。点击屏幕左上角附近的添加过滤器链接。在字段下拉菜单中输入值
    `type`。在操作字段中，将其设置为 `is`。在值字段中输入值 `request-incoming`，然后点击保存。接下来再次点击添加过滤器，并重复上述操作，但这次将字段设置为
    `app`，操作设置为 `is`，值设置为 `web-api`。
- en: For the Metrics section, leave it displaying the count, since it should display
    the number of requests and the matching log messages correlate one to one with
    real requests.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于指标部分，保留显示计数，因为它应显示请求的数量，并且匹配的日志消息与真实请求一一对应。
- en: For the Buckets section, it should be changed to group by time. Click the Add
    buckets link and select X-Axis. For the Aggregation drop-down menu, select Date
    Histogram. Click on the blue button with a play symbol above the Metrics section
    (it has a title of Apply changes), and the graph will update. The default setting
    of grouping by `@timestamp` with an automatic interval is fine.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于桶部分，应更改为按时间分组。点击添加桶链接，选择 X 轴。在聚合下拉菜单中，选择日期直方图。点击位于指标部分上方的带有播放符号的蓝色按钮（标题为应用更改），图表将更新。默认设置为按
    `@timestamp` 分组，并具有自动间隔，这是不错的。
- en: In the upper-right corner is a drop-down menu for changing the time range of
    the logs being queried. Click the drop-down menu and configure it to display logs
    from the last hour, and then click the large Refresh button to the right of the
    drop-down menu. If all goes to plan, your screen should look like [Figure 4-3](#fig_kibana-vis-complete).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在右上角有一个下拉菜单，可以更改查询日志的时间范围。点击下拉菜单，将其配置为显示最近一小时内的日志，然后点击右侧大的刷新按钮。如果一切顺利，您的屏幕应该看起来像
    [图 4-3](#fig_kibana-vis-complete)。
- en: '![A Kibana graph of timestamps over time](assets/dsnj_0403.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![一个随时间变化的 Kibana 时间戳图表](assets/dsnj_0403.png)'
- en: Figure 4-3\. Requests over time in Kibana
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. Kibana 中随时间变化的请求
- en: Once your graph is complete, click the Save link at the top of the Kibana screen.
    Name the visualization *web-api incoming requests*. Next, create a similar visualization
    but this time, set the *type* field to `request-outgoing` and name that visualization
    *web-api outgoing requests*. Finally, create a third visualization with a *type*
    field of `listen` and name it *web-api server starts*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完成图表后，点击 Kibana 屏幕顶部的保存链接。将可视化命名为 *web-api* *传入请求*。接下来，创建一个类似的可视化，但这次将 *type*
    字段设置为 `request-outgoing`，并将其命名为 *web-api* *传出请求*。最后，创建第三个可视化，将 *type* 字段设置为 `listen`，并将其命名为
    *web-api* *服务器启动*。
- en: Next, you’ll create a dashboard for these three visualizations. Select the third
    option in the sidebar titled Dashboard. Then, click Create new dashboard. A modal
    window will appear with your three visualizations in it. Click each visualization,
    and it will be added to the dashboard. Once you’ve added each visualization, dismiss
    the modal. Click the Save link at the top of the screen and save the dashboard
    as *web-api* *overview*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将为这三个可视化创建一个仪表板。选择侧边栏中的第三个选项标题为仪表板。然后，点击创建新仪表板。将出现一个模态窗口，其中包含您的三个可视化。点击每个可视化，它将被添加到仪表板中。添加完每个可视化后，关闭模态窗口。点击屏幕顶部的保存链接，并将仪表板保存为
    *web-api* *概述*。
- en: Congratulations! You’ve created a dashboard containing information extracted
    from your application.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已创建包含从应用程序提取的信息的仪表板。
- en: Running Ad-Hoc Queries
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行特定查询
- en: Sometimes you’ll need to run arbitrary queries against the data that’s being
    logged without a correlating dashboard. This is helpful in one-off debugging situations.
    In this section, you’ll write arbitrary queries in order to extract errors about
    the application.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有时您需要对正在记录的数据运行任意查询，而不需要相关的仪表板。这在一次性调试情况下非常有帮助。在本节中，您将编写任意查询，以提取关于应用程序错误的信息。
- en: 'Click the first tab in the left sidebar, the one titled Discover. This is a
    convenient playground for running queries without needing to commit them to a
    dashboard. By default, a listing of all recently received messages is displayed.
    Click inside of the Search field at the top of the screen. Then, type the following
    query into the search field and press Enter:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 单击左侧边栏中的第一个选项卡，标题为Discover。这是一个方便的查询运行场所，无需将其提交到仪表板中。默认情况下，屏幕顶部的搜索字段显示所有最近接收到的消息的列表。单击屏幕顶部的搜索字段内部。然后，输入以下查询到搜索字段并按Enter键：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The syntax of this query is written in the *Kibana* *Query Language* (KQL).
    Essentially, there are three clauses. It’s asking for logs belonging to the *web-api*
    application and whose *severity* levels are set to either *error* or *warn* (in
    other words, things that are very important).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询的语法是用*Kibana* *查询语言*（KQL）编写的。基本上，它包含三个子句。它要求获取属于*web-api*应用程序且*severity*级别设置为*error*或*warn*（换句话说，非常重要的事情）的日志。
- en: Click the arrow symbol next to one of the log entries in the list that follows.
    This will expand the individual log entry and allow you to view the entire JSON
    payload associated with the log. The ability to view arbitrary log messages like
    this is what makes logging so powerful. With this tool you’re now able to find
    all the errors being logged from the service.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 单击列表中某个日志条目旁边的箭头符号。这将展开单个日志条目，并允许您查看与日志关联的整个JSON负载。能够查看这种任意日志消息的能力是日志记录功能的强大所在。借助这个工具，您现在能够查找从服务中记录的所有错误。
- en: By logging more data, you’ll gain the ability to drill down into the details
    of specific error situations. For example, you might find that errors occur when
    a specific endpoint within an application is being hit under certain circumstances
    (like a user updating a recipe via `PUT /recipe` in a more full-featured application).
    With access to the stack trace, and enough contextual information about the requests,
    you’re then able to re-create the conditions locally, reproduce the bug, and come
    up with a fix.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过记录更多数据，您将能够深入了解特定错误情况的详细信息。例如，您可能会发现，在某些情况下（例如用户通过`PUT /recipe`更新食谱时），应用程序内的特定端点受到错误的影响。通过访问堆栈跟踪以及有关请求的足够上下文信息，您随后可以在本地重新创建条件，重现错误，并提出修复方案。
- en: Warning
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This section looks at transmitting logs from within an application, an inherently
    asynchronous operation. Unfortunately, logs generated when a process crashes might
    not be sent in time. Many deployment tools can read messages from *stdout* and
    transmit them on behalf of the application, which increases the likelihood of
    them being delivered.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了从应用程序内部传输日志，这是一个固有的异步操作。不幸的是，当进程崩溃时生成的日志可能不会及时发送。许多部署工具可以读取*stdout*中的消息，并代表应用程序将它们传输，这增加了它们被传送的可能性。
- en: This section looked at storing logs. Certainly, these logs can be used to display
    numeric information in graphs, but it isn’t necessarily the most efficient system
    for doing so since the logs store complex objects. The next section, [“Metrics
    with Graphite, StatsD, and Grafana”](#ch_monitoring_sec_metrics), looks at storing
    more interesting numeric data using a different set of tools.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了存储日志的问题。当然，这些日志可以用来在图表中显示数字信息，但这不一定是最有效的系统，因为日志存储复杂对象。下一节，[“使用Graphite、StatsD和Grafana进行指标监控”](#ch_monitoring_sec_metrics)，将使用不同的工具集来存储更有趣的数字数据。
- en: Metrics with Graphite, StatsD, and Grafana
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Graphite、StatsD和Grafana进行指标监控
- en: '[“Logging with ELK”](#ch_monitoring_sec_log) looked at transmitting logs from
    a running Node.js process. Such logs are formatted as JSON and are indexable and
    searchable on a per-log basis. This is perfect for reading messages related to
    a particular running process, such as reading variables and stack traces. However,
    sometimes you don’t necessarily care about individual pieces of numeric data,
    and instead you want to know about aggregations of data, usually as these values
    grow and shrink over time.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[“使用ELK进行日志记录”](#ch_monitoring_sec_log)介绍了从运行中的Node.js进程传输日志。这些日志格式化为JSON，可根据每个日志进行索引和搜索。这非常适合阅读与特定运行进程相关的消息，例如阅读变量和堆栈跟踪。然而，有时您可能并不一定关心单个数字数据的情况，而是想了解数据的聚合情况，通常这些值随着时间的推移而增长和减少。'
- en: This section looks at sending *metrics*. A metric is numeric data associated
    with time. This can include things like request rates, the number of *2XX* versus
    *5XX* HTTP responses, latency between the application and a backing service, memory
    and disk use, and even business stats like dollar revenue or cancelled payments.
    Visualizing such information is important to understanding application health
    and system load.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨发送*指标*。指标是与时间相关的数值数据。这可以包括请求速率、*2XX* 与 *5XX* HTTP 响应的数量、应用程序与后端服务之间的延迟、内存和磁盘使用情况，甚至像美元收入或取消支付等业务统计信息。可视化这些信息对于理解应用程序健康状况和系统负载至关重要。
- en: 'Much like in the logging section, a stack of tools will be used instead of
    a single one. However, this stack doesn’t really have a catchy acronym like ELK,
    and it’s fairly common to swap out different components. The stack considered
    here is that of *Graphite*, *StatsD*, and *Grafana*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在日志部分一样，这里将使用一组工具而不是单一的工具。然而，这个堆栈并没有像 ELK 那样引人注目的缩写，通常可以替换不同的组件。本节考虑的堆栈是 *Graphite*、*StatsD*
    和 *Grafana*：
- en: Graphite
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 石墨
- en: A combination of a service (*Carbon*) and time series database (*Whisper*).
    It also comes with a UI (*Graphite Web*), though the more powerful Grafana interface
    is often used.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一个服务（*Carbon*）和时间序列数据库（*Whisper*）的组合。它还配备了一个 UI（*Graphite Web*），不过通常使用更强大的 Grafana
    界面。
- en: StatsD
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD
- en: A daemon (built with Node.js) for collecting metrics. It can listen for stats
    over TCP or UDP before sending aggregations to a backend such as Graphite.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用 Node.js 构建的守护进程，用于收集指标。它可以监听 TCP 或 UDP 上的统计信息，然后将聚合发送到后端，如 Graphite。
- en: Grafana
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana
- en: A web service that queries time series backends (like Graphite) and displays
    information in configurable dashboards.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 web 服务，查询时间序列后端（如 Graphite），并在可配置的仪表板中显示信息。
- en: '[Figure 4-4](#fig_graphite) shows a diagram of these services and how they’re
    related. The Docker boundaries represent what the upcoming examples will use.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-4](#fig_graphite) 展示了这些服务的图示及其关系。Docker 的边界代表了接下来示例将使用的内容。'
- en: '![Administrator uses Grafana, Grafana talks to Graphite, Applications sends
    metrics to StatsD, and StatsD stores in Graphite](assets/dsnj_0404.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![管理员使用 Grafana，Grafana 与 Graphite 交互，应用程序将指标发送到 StatsD，StatsD 存储在 Graphite
    中](assets/dsnj_0404.png)'
- en: Figure 4-4\. Graphite, StatsD, and Grafana
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 石墨、StatsD 和 Grafana
- en: Much like in the logging section, these examples will transmit data using UDP.
    Due to the nature of metrics being rapidly produced, using UDP will help keep
    the application from getting overwhelmed.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在日志部分一样，这些示例将使用 UDP 传输数据。由于指标的快速生成特性，使用 UDP 将有助于防止应用程序被压倒。
- en: Running via Docker
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过 Docker 运行
- en: '[Example 4-6](#ex_metrics_docker) starts two separate Docker containers. The
    first one, *graphiteapp/graphite-statsd* contains StatsD and Graphite. Two ports
    from this container are exposed. The Graphite UI/API is exposed via port `:8080`,
    while the StatsD UDP metrics collector is exposed as `:8125`. The second, *grafana/grafana*,
    contains Grafana. A single port for the web interface, `:8000`, is exposed for
    this container.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-6](#ex_metrics_docker) 启动了两个独立的 Docker 容器。第一个容器 *graphiteapp/graphite-statsd*
    包含 StatsD 和 Graphite。该容器暴露了两个端口。Graphite UI/API 通过端口 `:8080` 暴露，而 StatsD UDP 指标收集器通过
    `:8125` 暴露。第二个容器 *grafana/grafana* 包含 Grafana。为该容器暴露了一个用于 web 界面的端口 `:8000`。'
- en: Example 4-6\. Running StatsD + Graphite, and Grafana
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-6\. 运行 StatsD + Graphite 和 Grafana
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Once the containers are up and running, open a web browser and visit the Grafana
    dashboard at [*http://localhost:8000/*](http://localhost:8000/). You’ll be asked
    to log in at this point. The default login credentials are *admin* / *admin*.
    Once you successfully log in, you’ll then be prompted to change the password to
    something else. This password will be used to administer Grafana, though it won’t
    be used in code.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦容器启动并运行，打开网页浏览器访问 Grafana 仪表板[*http://localhost:8000/*](http://localhost:8000/)。此时会要求您登录。默认登录凭据为
    *admin* / *admin*。成功登录后，您将被提示更改密码。这个密码将用于管理 Grafana，但不会在代码中使用。
- en: Once the password has been set, you’ll be taken to a wizard for configuring
    Grafana. The next step is to configure Grafana to communicate with the Graphite
    image. Click the Add Data Source button and then click the Graphite option. On
    the Graphite configuration screen, input the values displayed in [Table 4-1](#table_graphite_config).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了密码，您将进入一个向导来配置 Grafana。下一步是配置 Grafana 以与 Graphite 映像通信。点击“添加数据源”按钮，然后点击
    Graphite 选项。在 Graphite 配置屏幕上，输入 [表 4-1](#table_graphite_config) 中显示的值。
- en: Table 4-1\. Configuring Grafana to use Graphite
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4-1\. 配置 Grafana 使用 Graphite
- en: '| Name | Dist Node Graphite |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | Dist 节点 图形 |'
- en: '| URL | http://<LOCAL_IP>:8080 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| URL | http://<LOCAL_IP>:8080 |'
- en: '| Version | 1.1.x |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 版本 | 1.1.x |'
- en: Note
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Due to the way these Docker containers are being run, you won’t be able to use
    `localhost` for the `<LOCAL_IP>` placeholder. Instead, you’ll need to use your
    local IP address. If you’re on Linux, try running **`hostname -I`**, and if you’re
    on macOS, try running `ipconfig getifaddr en0`. If you’re running this on a laptop
    and your IP address changes, you’ll need to reconfigure the data source in Grafana
    to use the new IP address, or else you won’t get data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Docker 容器的运行方式，您将无法在 `<LOCAL_IP>` 占位符中使用 `localhost`。而是需要使用您的本地 IP 地址。如果您使用
    Linux，请尝试运行 **`hostname -I`**，如果您使用 macOS，请尝试运行 `ipconfig getifaddr en0`。如果您在笔记本电脑上运行，并且您的
    IP 地址发生了变化，您需要重新配置 Grafana 中的数据源，以使用新的 IP 地址，否则将无法获取数据。
- en: Once you’ve entered the data, click Save & Test. If you see the message “Data
    source is working,” then Grafana was able to talk to Graphite and you can click
    the Back button. If you get HTTP Error Bad Gateway, make sure the Graphite container
    is running and that the settings have been entered correctly.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据后，点击保存并测试。如果看到消息“数据源正在工作”，那么 Grafana 能够与 Graphite 通信，您可以点击返回按钮。如果收到 HTTP
    错误 Bad Gateway，请确保 Graphite 容器正在运行，并且已正确输入设置。
- en: Now that Graphite and Grafana are talking to each other, it’s time to modify
    one of the Node.js services to start sending metrics.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Graphite 和 Grafana 已经可以互相通信，是时候修改其中一个 Node.js 服务，开始发送指标了。
- en: Transmitting Metrics from Node.js
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Node.js 传输指标
- en: 'The [protocol used by StatsD](https://github.com/statsd/statsd) is extremely
    simple, arguably even simpler than the one used by Logstash UDP. An example message
    that increments a metric named `foo.bar.baz` looks like this:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[StatsD 使用的协议](https://github.com/statsd/statsd) 极其简单，可以说比 Logstash UDP 使用的协议还要简单。递增名为
    `foo.bar.baz` 的指标的示例消息如下：'
- en: '[PRE8]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Such interactions could very easily be rebuilt using the `dgram` module, like
    in the previous section. However, this code sample will make use of an existing
    package. There are a few out there, but this example uses the `statsd-client`
    package.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此类交互可以非常容易地使用 `dgram` 模块重新构建，就像前一节一样。但是，此代码示例将使用现有的包。市面上有几种包可供选择，但本示例使用 `statsd-client`
    包。
- en: Again, start by rebuilding a version of the consumer service. Copy the *web-api/consumer-http-basic.js*
    file created in [Example 1-7](ch01.html#ex_consumer) to *web-api/consumer-http-metrics.js*
    as a starting point. From there, modify the file to resemble [Example 4-7](#ex_statsd_consumer).
    Be sure to run the `npm install` command to get the required package as well.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 再次开始，通过重建消费者服务的一个版本来实现。将在 [示例 1-7](ch01.html#ex_consumer) 中创建的 *web-api/consumer-http-basic.js*
    文件复制到 *web-api/consumer-http-metrics.js* 作为起点。然后，在那里修改文件，使其类似于 [示例 4-7](#ex_statsd_consumer)。确保运行
    `npm install` 命令获取所需的包。
- en: Example 4-7\. *web-api/consumer-http-metrics.js* (first half)
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. *web-api/consumer-http-metrics.js*（上半部分）
- en: '[PRE9]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_observability_CO3-1)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO3-1)'
- en: Metric names are prefixed with `web-api`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 指标名称以 `web-api` 为前缀。
- en: '[![2](assets/2.png)](#co_observability_CO3-2)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO3-2)'
- en: A generic middleware that automatically tracks inbound requests.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的中间件，自动跟踪入站请求。
- en: '[![3](assets/3.png)](#co_observability_CO3-3)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO3-3)'
- en: This tracks the perceived timing to *recipe-api*.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这追踪到 *recipe-api* 的感知时机。
- en: '[![4](assets/4.png)](#co_observability_CO3-4)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_observability_CO3-4)'
- en: The number of outbound requests is also tracked.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 外发请求的数量也有记录。
- en: A few things are going on with this new set of changes. First, it requires the
    `statsd-client` package and configures a connection to the StatsD service listening
    at `localhost:8125`. It also configures the package to use a prefix value of `web-api`.
    This value represents the name of the service reporting the metrics (likewise,
    if you made similar changes to *recipe-api*, you’d set its prefix accordingly).
    Graphite works by using a hierarchy for naming metrics, so metrics sent from this
    service will all have the same prefix to differentiate them from metrics sent
    by another service.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这组新变更涉及几件事情。首先，它需要 `statsd-client` 包，并配置到监听 `localhost:8125` 的 StatsD 服务的连接。它还配置了包以使用
    `web-api` 的前缀值。此值代表报告指标的服务名称（同样，如果您对 *recipe-api* 进行类似更改，您将相应设置其前缀）。Graphite 通过使用层次结构来命名指标，因此从此服务发送的指标将都有相同的前缀，以区分其它服务发送的指标。
- en: The code makes use of a generic middleware provided by the `statsd-client` package.
    As the method name implies, it was originally designed for *Express*, but Fastify
    mostly supports the same middleware interface, so this application is able to
    reuse it. The first argument is another prefix name, and `inbound` implies that
    the metrics being sent here are associated with incoming requests.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 代码使用了`statsd-client`包提供的通用中间件。如方法名称所示，它最初是为*Express*设计的，但是 Fastify 大多数支持相同的中间件接口，因此该应用程序可以重用它。第一个参数是另一个前缀名称，`inbound`意味着此处发送的指标与传入请求相关联。
- en: Next, two values are manually tracked. The first is the amount of time the *web-api*
    perceives the *recipe-api* to have taken. Note that this time should always be
    longer than the time *recipe-api* believes the response took. This is due to the
    overhead of sending a request over the network. This timing value is written to
    a metric named `outbound.recipe-api.request-time`. The application also tracks
    how many requests are sent. This value is provided as `outbound.recipe-api.request-count`.
    You could even get more granular here. For example, for a production application,
    the status codes that the *recipe-api* responds with could also be tracked, which
    would allow an increased rate of failures to be visible.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，手动跟踪两个值。第一个是*web-api*认为*recipe-api*花费的时间量。请注意，此时间始终应比*recipe-api*认为响应花费的时间长。这是由于通过网络发送请求的开销。此时间值写入名为`outbound.recipe-api.request-time`的指标。该应用程序还跟踪发送的请求数量，提供为`outbound.recipe-api.request-count`。在此处甚至可以更加细化。例如，对于生产应用程序，还可以跟踪*recipe-api*响应的状态代码，这将使失败率增加可见。
- en: 'Next, run the following commands each in a separate terminal window. This will
    start your newly created service, run a copy of the producer, run Autocannon to
    get a stream of good requests, and also trigger some bad requests:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，分别在不同的终端窗口中运行以下命令。这将启动您新创建的服务，运行生产者的一个副本，运行 Autocannon 以获取一系列良好的请求流，并触发一些错误请求：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Those commands will generate a stream of data, which gets passed to StatsD before
    being sent to Graphite. Now that you have some data, you’re ready to create a
    dashboard to view it.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将生成一系列数据，通过 StatsD 传递到 Graphite。现在您已经有了一些数据，可以准备创建一个仪表板来查看它。
- en: Creating a Grafana Dashboard
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个 Grafana 仪表板
- en: As the owner of the *web-api* service, there are (at least) three different
    sets of metrics that should be extracted so that you can measure its health. This
    includes the incoming requests and, importantly, differentiating 200 from 500\.
    It also includes the amount of time that *recipe-api*, an upstream service, takes
    to reply. The final set of required information is the rate of requests to the
    *recipe-api* service. If you determine the *web-api* service is slow, you might
    use this information to discover that the *recipe-api* service is slowing it down.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 作为*web-api*服务的所有者，至少需要提取三组不同的指标来衡量其健康状况。这包括传入的请求，尤其是区分 200 和 500 的请求。还包括*recipe-api*作为上游服务回复所花费的时间。最后一组所需信息是*recipe-api*服务的请求速率。如果确定*web-api*服务运行缓慢，您可以利用这些信息发现*recipe-api*服务正在拖慢其速度。
- en: Switch back to your web browser with the Grafana interface. There is a large
    plus symbol in the sidebar; click it to be taken to the [New dashboard](http://localhost:8000/dashboard/new)
    screen. On this screen you’ll see a New Panel rectangle. Inside of it is an Add
    Query button. Click that button to be taken to the query editor screen.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回带有 Grafana 界面的网络浏览器。侧边栏有一个大加号符号；点击它进入[新仪表板](http://localhost:8000/dashboard/new)界面。在这个界面上，您会看到一个新面板矩形。里面有一个添加查询按钮。点击该按钮进入查询编辑器界面。
- en: On this new screen, you’ll see an empty graph at the top and inputs to describe
    the graph below. The UI lets you describe the query using two fields. The first
    is called Series and is where you can input the hierarchical metric name. The
    second field is called Functions. Both of these fields provide autocomplete for
    matching metric names. First, start with the Series field. Click the “select metric”
    text next to the Series label and then click `stats_count` from the drop-down
    menu. Then click “select metric” again and select `web-api`. Continue this for
    the values `inbound`, `response_code`, and finally `*` (the `*` is a wildcard
    and will match any value). At this point, the graph has been updated and should
    show two sets of entries.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在新屏幕上，你会看到顶部是一个空图表，下方是用于描述图表的输入框。界面允许你使用两个字段描述查询。第一个字段称为 Series，你可以在其中输入层级指标名称。第二个字段称为
    Functions。这两个字段都提供了匹配指标名称的自动完成功能。首先，从 Series 字段开始。点击 Series 标签旁边的“select metric”文本，然后从下拉菜单中选择
    `stats_count`。接着再次点击“select metric”，选择 `web-api`。依此类推，选择 `inbound`，`response_code`，最后选择
    `*`（`*` 是通配符，匹配任何值）。此时，图表已更新，应显示两组条目。
- en: The graph labels aren’t too friendly just yet. They’re displaying the entire
    hierarchy name instead of just the easy-to-read values 200 and 500\. A *Function*
    can be used to fix this. Click the plus sign next to the Functions label, then
    click Alias, and then click aliasByNode(). This will insert the function and also
    automatically provide a default argument of 4\. This is because the asterisk in
    the query is the 4th entry in the (zero-based) hierarchy metric name. The graph
    labels have been updated to display just 200 and 500.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 目前图表标签还不够友好。它们显示的是整个层次结构名称，而不是易于阅读的值 200 和 500。可以使用 *Function* 来解决这个问题。点击 Functions
    标签旁边的加号，然后点击 Alias，再点击 aliasByNode()。这将插入函数，并自动提供一个默认参数 4。这是因为查询中的星号在层次指标名称中是第四个条目（从零开始计数）。图表标签已更新，只显示
    200 和 500。
- en: 'In the upper-right corner of the panel with the Series and Functions fields,
    there’s a pencil icon with a tooltip titled Toggle text edit mode. Click that,
    and the graphical entry will change into a text version. This is helpful for quickly
    writing a query. The value you should have looks like the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Series 和 Functions 字段面板右上角，有一个铅笔图标，带有标题为 Toggle text edit mode 的工具提示。点击它，图形条目将转换为文本版本。这对于快速编写查询非常有帮助。你应该得到如下数值：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the left column, click the gear icon labeled General. On this screen you’re
    able to modify generic settings about this particular graph. Click the Title field,
    and input a value of Incoming Status Codes. Once that’s done, click the large
    arrow in the upper-left corner of the screen. This will take you from the panel
    editor screen and back to the dashboard edit screen. At this point, your dashboard
    will have a single panel.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧列，点击标题为 General 的齿轮图标。在这个屏幕上，你可以修改关于此图表的通用设置。点击 Title 字段，并输入值 Incoming Status
    Codes。完成后，点击屏幕左上角的大箭头。这将从面板编辑屏幕返回到仪表板编辑屏幕。此时，你的仪表板将只有一个面板。
- en: 'Next, click the Add panel button in the upper-right corner of the screen and
    then click the Add query button again. This will allow you to add a second panel
    to the dashboard. This next panel will track the time it takes to query the *recipe-api*.
    Create the appropriate Series and Functions entries to reproduce the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在屏幕右上角点击 Add panel 按钮，然后再次点击 Add query 按钮。这将允许你向仪表板添加第二个面板。下一个面板将跟踪查询 *recipe-api*
    所需的时间。创建适当的 Series 和 Functions 条目以重现以下内容：
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: StatsD is generating some of these metric names for you. For example, `stats.timers`
    is a StatsD prefix, `web-api.outbound.recipe-api.request-time` is provided by
    the application, and the timing-related metric names under that (such as `upper_90`)
    are again calculated by StatsD. In this case, the query is looking at TP90 timing
    values.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: StatsD 会为你生成一些指标名称。例如，`stats.timers` 是 StatsD 的前缀，`web-api.outbound.recipe-api.request-time`
    是应用程序提供的，而该前缀下的与计时相关的指标名称（如 `upper_90`）则是由 StatsD 计算得出的。在这种情况下，查询关注的是 TP90 的计时数值。
- en: Since this graph measures time and is not a generic counter, the units should
    be modified as well (this information is measured in milliseconds). Click the
    second tab on the left, with a tooltip of Visualization. Then, scroll down the
    section labeled Axes, find the group titled Left Y, and then click the Unit drop-down
    menu. Click Time, then click milliseconds (ms). The graph will then be updated
    with proper units.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此图表测量时间而不是通用计数器，因此应修改单位（此信息以毫秒为单位）。点击左侧的第二个选项卡，其工具提示为“可视化”。然后，向下滚动到标有 Axes
    的部分，找到标题为 Left Y 的组，并点击单位下拉菜单。选择时间，然后点击毫秒（ms）。图表将随之使用正确的单位进行更新。
- en: Click the third General tab again and set the panel’s title to Outbound Service
    Timing. Click the back arrow again to return to the dashboard edit screen.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 再次点击第三个 General 标签，将面板标题设置为 Outbound Service Timing。再次点击返回箭头，返回到仪表板编辑屏幕。
- en: 'Finally, click the Add panel button again and go through creating a final panel.
    This panel will be titled Outbound Request Count, won’t need any special units,
    and will use the following query:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次点击添加面板按钮，并开始创建最后一个面板。此面板标题为 Outbound Request Count，不需要任何特殊单位，并使用以下查询：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Click the back button a final time to return to the dashboard editor screen.
    In the upper-right corner of the screen, click the Save dashboard icon, give the
    dashboard a name of Web API Overview, and save the dashboard. The dashboard is
    now saved and will have a URL associated with it. If you were using an instance
    of Grafana permanently installed for your organization, this URL would be a permalink
    that you could provide to others and would make a great addition to your project’s
    README.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后点击返回按钮，返回仪表板编辑屏幕。在屏幕右上角，点击保存仪表板图标，将仪表板命名为 Web API 概述，并保存仪表板。仪表板现在已保存，并将与一个
    URL 相关联。如果您正在为您的组织永久安装的 Grafana 实例，这个 URL 将是一个永久链接，您可以将其提供给他人，并将其添加到您项目的 README
    中。
- en: Feel free to drag the panels around and resize them until you get something
    that is aesthetically pleasing. In the upper right corner of the screen, you can
    also change the time range. Set it to “Last 15 minutes,” since you likely don’t
    have data much older than that. Once you’re done, your dashboard should look something
    like [Figure 4-5](#fig_grafana).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 随意拖动面板并调整大小，直到达到美观的效果。在屏幕右上角，您还可以更改时间范围。将其设置为“最近 15 分钟”，因为您可能没有比这更早的数据。完成后，您的仪表板应该看起来类似于
    [图 4-5](#fig_grafana)。
- en: '![Grafana dashboard showing Incoming Status Codes, Outbound Service Timing,
    and Outbound Request Count](assets/dsnj_0405.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![Grafana 仪表板显示入站状态码、出站服务时间和出站请求计数](assets/dsnj_0405.png)'
- en: Figure 4-5\. Completed Grafana dashboard
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 完成的 Grafana 仪表板
- en: Node.js Health Indicators
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Node.js 健康指标
- en: There is some generic health information about a running Node.js process that
    is also worth collecting for the dashboard. Modify your *web-api/consumer-http-metrics.js*
    file by adding the code from [Example 4-8](#ex_statsd_consumer_extra) to the end
    of the file. Restart the service and keep an eye on the data that is being generated.
    These new metrics represent values that can increase or decrease over time and
    are better represented as *Gauges*.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有关正在运行的 Node.js 进程的一些通用健康信息，也值得收集到仪表板中。通过在 *web-api/consumer-http-metrics.js*
    文件末尾添加来自 [示例 4-8](#ex_statsd_consumer_extra) 的代码来修改您的文件。重新启动服务，并关注正在生成的数据。这些新的指标表示随时间而变化的值，更适合表示为
    *Gauges*。
- en: Example 4-8\. *web-api/consumer-http-metrics.js* (second half)
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. *web-api/consumer-http-metrics.js*（后半部分）
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_observability_CO4-1)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO4-1)'
- en: Number of connections to server
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 与服务器的连接数
- en: '[![2](assets/2.png)](#co_observability_CO4-2)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO4-2)'
- en: Process heap utilization
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 进程堆利用率
- en: '[![3](assets/3.png)](#co_observability_CO4-3)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO4-3)'
- en: V8 heap utilization
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: V8 堆利用率
- en: '[![4](assets/4.png)](#co_observability_CO4-4)'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_observability_CO4-4)'
- en: Open file descriptors, ironically using a file descriptor
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 打开文件描述符，具有讽刺意味的使用文件描述符
- en: '[![5](assets/5.png)](#co_observability_CO4-5)'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_observability_CO4-5)'
- en: Event loop lag
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 事件循环延迟
- en: This code will poll the Node.js underbelly every 10 seconds for key information
    about the process. As an exercise of your newfound Grafana skills, create five
    new dashboards containing this newly captured data. In the metric namespace hierarchy,
    the guage metrics begin with `stats.gauges`, while the timer starts with `stats.timers`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将每隔 10 秒轮询 Node.js 的核心，获取有关进程的关键信息。作为对您新发现的 Grafana 技能的练习，创建五个新仪表板，包含这些新捕获的数据。在度量命名空间层次结构中，仪表度量从
    `stats.gauges` 开始，而计时器从 `stats.timers` 开始。
- en: The first set of data, provided as `server.conn`, is the number of active connections
    to the web server. Most Node.js web frameworks expose this value in some manner;
    check out the documentation for your framework of choice.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组数据，以 `server.conn` 提供，是到 Web 服务器的活动连接数。大多数 Node.js Web 框架以某种方式公开此值；请查看您选择的框架的文档。
- en: Information about the process memory usage is also captured. This is being recorded
    as two values, `server.memory.used` and `server.memory.total`. When creating a
    graph for these values, their unit should be set to Data/Bytes, and Grafana is
    smart enough to display more specific units like MB. A very similar panel could
    then be made based on the V8 heap size and limit.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 还捕获了有关进程内存使用情况的信息。这被记录为两个值，`server.memory.used` 和 `server.memory.total`。在为这些值创建图表时，它们的单位应设置为数据/字节，Grafana
    足够智能以显示更具体的单位，如 MB。然后可以基于 V8 堆大小和限制创建一个非常类似的面板。
- en: The event loop lag metric displays how long it takes the application to call
    a function that was scheduled to run as early as zero milliseconds from the time
    `setTimeout()` was called. This graph should display the value in milliseconds.
    A healthy event loop should have a number between zero and two. Overwhelmed services
    might start taking tens of milliseconds.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 事件循环滞后指标显示应用程序调用函数所需的时间，该函数被安排在从调用 `setTimeout()` 开始到零毫秒的时间内运行。该图表应以毫秒为单位显示值。一个健康的事件循环应该在零到二之间。不堪重负的服务可能会开始花费数十毫秒。
- en: Finally, the number of open file descriptors can indicate a leak in a Node.js
    application. Sometimes files will be opened but will never be closed, and this
    can lead to consumption of server resources and result in a process crash.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，打开的文件描述符数量可能表明 Node.js 应用程序存在泄漏。有时文件会被打开但不会被关闭，这可能导致服务器资源的消耗，并导致进程崩溃。
- en: Once you’ve added the new panels, your dashboard may then resemble [Figure 4-6](#fig_grafana_advanced).
    Save the modified dashboard so that you don’t lose your changes.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 添加了新面板后，您的仪表板可能会像 [图 4-6](#fig_grafana_advanced) 那样。保存修改后的仪表板，以防丢失您的更改。
- en: '![Grafana dashboard showing server memory usage, V8 memory usage, and open
    file descriptors](assets/dsnj_0406.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![Grafana 仪表板显示服务器内存使用情况、V8 内存使用情况和打开文件描述符](assets/dsnj_0406.png)'
- en: Figure 4-6\. Updated Grafana dashboard
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 更新后的 Grafana 仪表板
- en: This section only covers the basics of what can be done with the StatsD, Graphite,
    and Grafana stack. There are many query functions that haven’t been covered, including
    other forms of visualizations, how to manually color individual time series entries
    (like green for 2XX, yellow for 4XX, and red for 5XX), and so on.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本节仅涵盖了使用 StatsD、Graphite 和 Grafana 堆栈的基础知识。还有许多未涵盖的查询功能，包括其他形式的可视化，如如何手动为单个时间序列条目着色（例如对于
    2XX 使用绿色，对于 4XX 使用黄色，对于 5XX 使用红色）等等。
- en: Distributed Request Tracing with Zipkin
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Zipkin 进行分布式请求追踪
- en: '[“Logging with ELK”](#ch_monitoring_sec_log) looked at storing logs from a
    Node.js process. Such logs contain information about the internal operations of
    a process. Likewise, [“Metrics with Graphite, StatsD, and Grafana”](#ch_monitoring_sec_metrics)
    looked at storing numeric metrics. These metrics are useful for looking at numeric
    data in aggregate about an application, such as throughput and failure rates for
    an endpoint. However, neither of these tools allow for associating a specific
    external request with all the internal requests it may then generate.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[“使用 ELK 进行日志记录”](#ch_monitoring_sec_log) 研究了存储来自 Node.js 进程的日志。这些日志包含有关进程内部操作的信息。同样，[“使用
    Graphite、StatsD 和 Grafana 进行度量”](#ch_monitoring_sec_metrics) 研究了存储数字度量。这些度量对于查看应用程序的数字数据总体，如端点的吞吐量和失败率，非常有用。然而，这些工具都不允许将特定外部请求与其可能生成的所有内部请求关联起来。'
- en: Consider, for example, a slightly more complex version of the services covered
    so far. Instead of just a *web-api* and a *recipe-api* service, there’s an additional
    *user-api* and a *user-store* service. The *web-api* will still call the *recipe-api*
    service as before, but now the *web-api* will also call the *user-api* service,
    which will in turn call the *user-store* service. In this scenario, if any one
    of the services produces a 500 error, that error will bubble up and the overall
    request will fail with a 500\. How would you find the cause of a specific error
    with the tools used so far?
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑到到目前为止涵盖的服务的稍微复杂版本。不仅有 *web-api* 和 *recipe-api* 服务，还有额外的 *user-api* 和 *user-store*
    服务。*web-api* 仍然像以前一样调用 *recipe-api* 服务，但现在 *web-api* 还会调用 *user-api* 服务，后者将进一步调用
    *user-store* 服务。在这种情况下，如果任何一个服务产生500错误，该错误将传播并导致整体请求失败，显示为500错误。到目前为止使用的工具如何找到特定错误的原因？
- en: Well, if you know that an error occurred on Tuesday at 1:37 P.M., you might
    be tempted to look through logs stored in ELK between the time of 1:36 P.M. and
    1:38 P.M. Goodness knows I’ve done this myself. Unfortunately, if there is a high
    volume of logs, this could mean sifting through thousands of individual log entries.
    Worse, other errors happening at the same time can “muddy the water,” making it
    hard to know which logs are actually associated with the erroneous request.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道一个错误发生在周二下午1:37，你可能会想要查看ELK存储的日志，时间从下午1:36到1:38。老实说，我自己也做过这件事。不幸的是，如果日志量很大，这可能意味着要筛选成千上万条单独的日志条目。更糟糕的是，同时发生的其他错误可能会“混淆视听”，使人难以知道哪些日志实际上与错误请求相关联。
- en: At a very basic level, requests made deeper within an organization can be associated
    with a single incoming external request by passing around a *request ID*. This
    is a unique identifier that is generated when the first request is received, which
    is then somehow passed between upstream services. Then, any logs associated with
    this request will contain some sort of `request_id` field, which can then be filtered
    using Kibana. This approach solves the associated request conundrum but loses
    information about the hierarchy of related requests.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常基础的层面上，通过传递 *request ID* 可以将组织内更深的请求与单个外部入站请求关联起来。这是在收到第一个请求时生成的唯一标识符，然后在上游服务之间传递。然后，与此请求相关的任何日志将包含某种
    `request_id` 字段，可以使用Kibana进行过滤。这种方法解决了关联请求的难题，但失去了有关相关请求层次结构的信息。
- en: '*Zipkin*, sometimes referred to as *OpenZipkin*, is a tool that was created
    to alleviate situations just like this one. Zipkin is a service that runs and
    exposes an HTTP API. This API accepts JSON payloads describing request metadata,
    as they are both sent by clients and received by servers. Zipkin also defines
    a set of headers that are passed from client to server. These headers allow processes
    to associate outgoing requests from a client with incoming requests to a server.
    Timing information is also sent, which then allows Zipkin to display a graphical
    timeline of a request hierarchy.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*Zipkin*，有时也称为 *OpenZipkin*，是一种旨在缓解类似情况的工具。Zipkin是一个运行并公开HTTP API的服务。此API接受描述请求元数据的JSON负载，因为它们由客户端发送和服务器接收。Zipkin还定义了一组从客户端传递到服务器的标头。这些标头允许进程将客户端的出站请求与服务器的入站请求关联起来。还会发送时间信息，这样Zipkin可以显示请求层次结构的图形时间线。'
- en: How Does Zipkin Work?
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何使用 Zipkin？
- en: In the aforementioned scenario with the four services, the relationship between
    services transpires over four requests. When this happens, seven messages will
    be sent to the Zipkin service. [Figure 4-7](#fig_zipkin_overview) contains a visualization
    of the service relationships, the passed messages, and the additional headers.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在前述涉及四个服务的情景中，服务之间的关系经过四次请求。在这种情况下，将发送七条消息到Zipkin服务。[图4-7](#fig_zipkin_overview)
    包含服务关系、传递消息和额外标头的可视化。
- en: '![Dependency graph between four services](assets/dsnj_0407.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![四个服务之间的依赖关系图](assets/dsnj_0407.png)'
- en: Figure 4-7\. Example requests and Zipkin data
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-7。示例请求和Zipkin数据
- en: One concept that has been repeated a few times so far in this book is that a
    client will perceive one latency of a request, while a server will perceive another
    latency. A client will always determine that a request takes longer than the server.
    This is due to the time it takes a message to be sent over the network, plus other
    things that are hard to measure, such as a web server package automatically parsing
    a JSON request before user code can start measuring time.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书迄今为止多次重复的一个概念是，客户端将感知请求的一个延迟，而服务器将感知另一个延迟。客户端始终认为请求时间比服务器长。这是由于消息在网络上传输的时间，以及其他难以测量的因素，例如
    Web 服务器包在用户代码开始测量时间之前自动解析 JSON 请求所需的时间。
- en: Zipkin allows you to measure the difference in opinion between client and server.
    This is why the four requests in the example situation, marked as solid arrows
    in [Figure 4-7](#fig_zipkin_overview), result in seven different messages being
    sent to Zipkin. The first message, terminating with S1, only contains a *server
    message*. In this case, the third-party client isn’t reporting its perceived time,
    so there’s just the server message. For the three requests terminating in S2,
    S3, and S4, there is a correlating *client message*, namely C2, C3, and C4.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin 允许您测量客户端和服务器之间的意见差异。这就是为什么在示例情况中的四个请求，如 [Figure 4-7](#fig_zipkin_overview)
    中标记为实线箭头，会导致发送到 Zipkin 的七条不同消息。以 S1 结尾的第一条消息仅包含一个 *服务器消息*。在这种情况下，第三方客户端未报告其感知时间，因此只有服务器消息。对于以
    S2、S3 和 S4 结尾的三个请求，存在相应的 *客户端消息*，即 C2、C3 和 C4。
- en: 'The different client and server messages can be sent from the different instances,
    asynchronously, and can be received in any order. The Zipkin service will then
    stitch them each together and visualize the request hierarchy using the Zipkin
    web UI. The C2 message will look something like this:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的客户端和服务器消息可以从不同的实例异步发送，并且可以以任何顺序接收。然后，Zipkin 服务将它们拼接在一起，并使用 Zipkin Web UI
    可视化请求层次结构。C2 消息看起来会像这样：
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These messages can be queued up by an application and occasionally flushed in
    batches to the Zipkin service, which is why the root JSON entry is an array. In
    [Example 4-9](#ex_zipkin_consumer), only a single message is being transmitted.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些消息可以被应用程序排队，并偶尔以批处理方式刷新到 Zipkin 服务，这就是为什么根 JSON 条目是一个数组的原因。在 [Example 4-9](#ex_zipkin_consumer)
    中，仅传输了单条消息。
- en: The client message and server message pairs will end up containing the same
    `id`, `traceId`, and `parentId` identifiers. The `timestamp` field represents
    the time when the client or server first perceived the request to start, and the
    `duration` is how long the service thought the request lasted. Both of these fields
    are measured in microseconds. The Node.js *wall clock*, attainable via `Date.now()`,
    only has millisecond accuracy, so it’s common to multiply that value by 1,000.^([1](ch04.html#idm46291188172088))
    The `kind` field is set to either `CLIENT` or `SERVER`, depending on which side
    of the request is being logged. The `name` field represents a name for the endpoint
    and should have a finite set of values (in other words, don’t use an identifier).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端消息和服务器消息对最终包含相同的 `id`、`traceId` 和 `parentId` 标识符。`timestamp` 字段表示客户端或服务器首次感知到请求开始的时间，`duration`
    表示服务认为请求持续的时间。这两个字段都是以微秒为单位进行测量。Node.js 的 *wall clock*，可通过 `Date.now()` 获取，只有毫秒级的精度，因此通常将其乘以
    1,000。^([1](ch04.html#idm46291188172088)) `kind` 字段设置为 `CLIENT` 或 `SERVER`，取决于记录请求的哪一侧。`name`
    字段表示端点的名称，应具有有限的值集（换句话说，不要使用标识符）。
- en: The `localEndpoint` field represents the service sending the message (the server
    with a `SERVER` message or the client with a `CLIENT` message). The service provides
    its own name in here, the port it’s listening on, and its own IP address. The
    `remoteEndpoint` field contains information about the other service (a `SERVER`
    message probably won’t know the client’s `port`, and likely won’t even know the
    client’s `name`).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`localEndpoint` 字段表示发送消息的服务（具有 `SERVER` 消息的服务器或具有 `CLIENT` 消息的客户端）。服务在此提供自己的名称，监听的端口以及自己的
    IP 地址。`remoteEndpoint` 字段包含有关其他服务的信息（`SERVER` 消息可能不知道客户端的 `port`，甚至可能不知道客户端的 `name`）。'
- en: The `tags` field contains metadata about the request. In this example, information
    about the HTTP request is provided as `http.method` and `http.path`. With other
    protocols, different metadata would be attached, such as a gRPC service and method
    name.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`tags` 字段包含有关请求的元数据。在此示例中，提供了关于 HTTP 请求的信息，如 `http.method` 和 `http.path`。对于其他协议，会附加不同的元数据，例如
    gRPC 服务和方法名称。'
- en: The identifiers sent in the seven different messages have been re-created in
    [Table 4-2](#table_zipkin_labels).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Table 4-2](#table_zipkin_labels)中重新创建了七条不同消息中发送的标识符。
- en: Table 4-2\. Values reported from [Figure 4-7](#fig_zipkin_overview)
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Table 4-2\. 从[Figure 4-7](#fig_zipkin_overview)报告的值
- en: '| Message | `id` | `parentId` | `traceId` | `kind` |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Message | `id` | `parentId` | `traceId` | `kind` |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| S1 | 110 | N/A | 100 | SERVER |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| S1 | 110 | N/A | 100 | SERVER |'
- en: '| C2 | 111 | 110 | 100 | CLIENT |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| C2 | 111 | 110 | 100 | CLIENT |'
- en: '| S2 | 111 | 110 | 100 | SERVER |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 111 | 110 | 100 | SERVER |'
- en: '| C3 | 121 | 110 | 100 | CLIENT |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 121 | 110 | 100 | CLIENT |'
- en: '| S3 | 121 | 110 | 100 | SERVER |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| S3 | 121 | 110 | 100 | SERVER |'
- en: '| C4 | 122 | 121 | 100 | CLIENT |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 122 | 121 | 100 | CLIENT |'
- en: '| S4 | 122 | 121 | 100 | SERVER |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| S4 | 122 | 121 | 100 | SERVER |'
- en: 'Apart from the messages sent to the server, the other important part of Zipkin
    is the metadata that is sent from client to server. Different protocols have different
    standards for sending this metadata. With HTTP, the metadata is sent via headers.
    These headers are provided by C2, C3, and C4 and are received by S2, S3, and S4\.
    Each of these headers has a different meaning:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 除了发送到服务器的消息外，Zipkin的另一个重要部分是从客户端到服务器发送的元数据。不同的协议对发送此元数据有不同的标准。对于HTTP，元数据通过标头发送。这些标头由C2、C3和C4提供，并由S2、S3和S4接收。这些标头每个都有不同的含义：
- en: '`X-B3-TraceId`'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-B3-TraceId`'
- en: Zipkin refers to all related requests as a *trace*. This value is Zipkin’s concept
    of a *request ID*. This value is passed between all related requests, unchanged.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin将所有相关请求称为一个*trace*。这个值是Zipkin的*request ID*的概念。这个值在所有相关请求之间传递，不变。
- en: '`X-B3-SpanId`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-B3-SpanId`'
- en: A *span* represents a single request, as seen from both a client and a server
    (like C3/S3). Both the client and server will send a message using the same span
    ID. There can be multiple spans in a trace, forming a tree structure.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*span*代表一个单独的请求，从客户端和服务器的视角看（如C3/S3）。客户端和服务器将使用相同的span ID发送消息。一个跟踪中可以有多个span，形成树结构。
- en: '`X-B3-ParentSpanId`'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-B3-ParentSpanId`'
- en: A *parent span* is used for associating a child span with a parent span. This
    value is missing for the originating external request but is present for deeper
    requests.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '*parent span*用于将子span与父span关联起来。对于起始的外部请求，该值缺失，但对于更深层次的请求，该值存在。'
- en: '`X-B3-Sampled`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-B3-Sampled`'
- en: This is a mechanism used for determining if a particular trace should be reported
    to Zipkin. For example, an organization may choose to track only 1% of requests.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于确定是否将特定跟踪报告给Zipkin的机制。例如，组织可以选择仅跟踪1%的请求。
- en: '`X-B3-Flags`'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`X-B3-Flags`'
- en: This can be used to tell downstream services that this is a debug request. Services
    are encouraged to then increase their logging verbosity.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来告诉下游服务，这是一个调试请求。建议服务随后增加其日志详细程度。
- en: Essentially, each service creates a new span ID for each outgoing request. The
    current span ID is then provided as the parent ID in the outbound request. This
    is how the hierarchy of relationships is formed.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务为每个出站请求创建一个新的span ID。然后，当前span ID作为出站请求中的父ID提供。这就是关系层次结构形成的方式。
- en: Now that you understand the intricacies of Zipkin, it’s time to run a local
    copy of the Zipkin service and modify the applications to interact with it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Zipkin的复杂性，是时候运行Zipkin服务的本地副本并修改应用程序与其交互了。
- en: Running Zipkin via Docker
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Docker运行Zipkin
- en: Again, Docker provides a convenient platform for running the service. Unlike
    the other tools covered in this chapter, Zipkin provides an API and a UI using
    the same port. Zipkin uses a default port of `9411` for this.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了一个便利的平台来运行服务。与本章涵盖的其他工具不同，Zipkin提供了一个API和一个使用相同端口的UI。Zipkin默认使用`9411`端口。
- en: Run this command to download and start the Zipkin service:^([2](ch04.html#idm46291188111672))
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令下载并启动Zipkin服务：^([2](ch04.html#idm46291188111672))
- en: '[PRE16]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Transmitting Traces from Node.js
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Node.js传输跟踪
- en: For this example, you’re going to again start by modifying an existing application.
    Copy the *web-api/consumer-http-basic.js* file created in [Example 1-7](ch01.html#ex_consumer)
    to *web-api/consumer-http-zipkin.js* as a starting point. Modify the file to look
    like the code in [Example 4-9](#ex_zipkin_consumer).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本例，您将再次从修改现有应用程序开始。将在[Example 1-7](ch01.html#ex_consumer)中创建的*web-api/consumer-http-basic.js*文件复制到*web-api/consumer-http-zipkin.js*，作为起点。修改文件以看起来像[Example 4-9](#ex_zipkin_consumer)中的代码。
- en: Example 4-9\. *web-api/consumer-http-zipkin.js*
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-9\. *web-api/consumer-http-zipkin.js*
- en: '[PRE17]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](assets/1.png)](#co_observability_CO5-1)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO5-1)'
- en: The `zipkin-lite` package is required and instantiated.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 需要导入并实例化`zipkin-lite`包。
- en: '[![2](assets/2.png)](#co_observability_CO5-2)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO5-2)'
- en: '*web-api* accepts outside requests and can generate trace IDs.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '*web-api* 接受外部请求并可以生成跟踪ID。'
- en: '[![3](assets/3.png)](#co_observability_CO5-3)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO5-3)'
- en: Hooks are called when requests start and finish.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求开始和完成时，会调用钩子。
- en: '[![4](assets/4.png)](#co_observability_CO5-4)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_observability_CO5-4)'
- en: Each endpoint will need to specify its name.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 每个端点都需要指定其名称。
- en: '[![5](assets/5.png)](#co_observability_CO5-5)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_observability_CO5-5)'
- en: Outbound requests are manually instrumented.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 出站请求需要手动进行仪器化。
- en: Note
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These examples use the `zipkin-lite` package. This package requires manual instrumentation,
    which is a fancy way of saying that you, the developer, must call different hooks
    to interact with the package. I chose it for this project to help demonstrate
    the different parts of the Zipkin reporting process. For a production app, the
    official Zipkin package, [`zipkin`](https://www.npmjs.com/package/zipkin), would
    make for a better choice.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例使用了`zipkin-lite`包。此包需要手动仪器化，即开发者必须调用不同的钩子来与该包进行交互。我选择它来帮助演示Zipkin报告过程的不同部分。对于生产应用程序，官方的Zipkin包[`zipkin`](https://www.npmjs.com/package/zipkin)将是更好的选择。
- en: The consumer service represents the first service that an external client will
    communicate with. Because of this, the `init` configuration flag has been enabled.
    This will allow the service to generate a new trace ID. In theory, a reverse proxy
    can be configured to also generate initial identifier values. The `serviceName`,
    `servicePort`, and `serviceIp` fields are each used for reporting information
    about the running service to Zipkin.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者服务代表了外部客户端将要通信的第一个服务。因此，已启用`init`配置标志。这将允许服务生成一个新的跟踪ID。理论上，可以配置反向代理来生成初始标识符值。`serviceName`、`servicePort`和`serviceIp`字段各自用于向Zipkin报告有关正在运行的服务的信息。
- en: The `onRequest` and `onResponse` hooks allow the `zipkin-lite` package to interpose
    on requests. The `onRequest` handler runs first. It records the time the request
    starts and injects a `req.zipkin` property that can be used throughout the life
    cycle of the request. Later, the `onResponse` handler is called. This then calculates
    the overall time the request took and sends a `SERVER` message to the Zipkin server.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`zipkin-lite`包允许在请求中插入`onRequest`和`onResponse`钩子。首先运行`onRequest`处理程序，记录请求开始的时间并注入`req.zipkin`属性，该属性可以在请求的整个生命周期中使用。随后调用`onResponse`处理程序，计算请求的总体时间并向Zipkin服务器发送`SERVER`消息。'
- en: Within a request handler, two things need to happen. The first is that the name
    of the endpoint has to be set. This is done by calling `req.zipkin.setName()`.
    The second is that for each outbound request that is sent, the appropriate headers
    need to be applied and the time the request took should be calculated. This is
    done by first calling `req.zipkin.prepare()`. When this is called, another time
    value is recorded and a new span ID is generated. This ID and the other necessary
    headers are provided in the returned value, which is assigned here to the variable
    `zreq`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求处理程序中，需要完成两件事情。第一件事是设置端点的名称，通过调用`req.zipkin.setName()`来完成。第二件事是对发送的每个出站请求应用适当的头部，并计算请求所花费的时间。首先调用`req.zipkin.prepare()`来完成这一步。当调用此方法时，记录另一个时间值并生成新的span
    ID。将这个ID和其他必要的头部信息提供给返回值，将其分配给变量`zreq`。
- en: These headers are then provided to the request via `zreq.headers`. Once the
    request is complete, a call to `zreq.complete()` is made, passing in the request
    method and URL. Once this happens, the overall time taken is calculated, and the
    `CLIENT` message is then sent to the Zipkin server.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过`zreq.headers`将这些头部提供给请求。一旦请求完成，调用`zreq.complete()`，传入请求方法和URL。完成后，计算总共花费的时间，并向Zipkin服务器发送`CLIENT`消息。
- en: Next up, the producing service should also be modified. This is important because
    not only should the timing as perceived by the client be reported (*web-api* in
    this case), but the timing from the server’s point of view (*recipe-api*) should
    be reported as well. Copy the *recipe-api/producer-http-basic.js* file created
    in [Example 1-6](ch01.html#ex_producer) to *recipe-api/producer-http-zipkin.js*
    as a starting point. Modify the file to look like the code in [Example 4-10](#ex_zipkin_producer).
    Most of the file can be left as is, so only the required changes are displayed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，生产服务也应该进行修改。这很重要，因为不仅应该报告客户端感知的时间（在本例中为 *web-api*），还应该报告服务器视角的时间（*recipe-api*）。将
    [示例 1-6](ch01.html#ex_producer) 中创建的 *recipe-api/producer-http-basic.js* 文件复制到
    *recipe-api/producer-http-zipkin.js* 作为起点。修改文件使其看起来像 [示例 4-10](#ex_zipkin_producer)
    中的代码。大部分文件可以保持不变，因此只显示必要的更改部分。
- en: Example 4-10\. *recipe-api/producer-http-zipkin.js* (truncated)
  id: totrans-266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-10\. *recipe-api/producer-http-zipkin.js*（已截断）
- en: '[PRE18]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Example 4-10](#ex_zipkin_producer) doesn’t act as a root service, so the `init`
    configuration flag has been omitted. If it receives a request directly, it won’t
    generate a trace ID, unlike the *web-api* service. Also, note that the same `req.zipkin.prepare()`
    method is available in this new *recipe-api* service, even though the example
    isn’t using it. When implementing Zipkin within services you own, you’ll want
    to pass the Zipkin headers to as many upstream services as you can.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 4-10](#ex_zipkin_producer) 不作为根服务，因此忽略了 `init` 配置标志。如果直接收到请求，它不会生成追踪 ID，不像
    *web-api* 服务那样。同时，请注意，即使示例未使用，相同的 `req.zipkin.prepare()` 方法也适用于这个新的 *recipe-api*
    服务。在实现你拥有的服务中引入 Zipkin 时，你应该尽可能地向上游服务传递 Zipkin 标头。'
- en: Be sure to run the **`npm install zipkin-lite@0.1`** command in both project
    directories.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保在两个项目目录中运行 **`npm install zipkin-lite@0.1`** 命令。
- en: 'Once you’ve created the two new service files, run them and then generate a
    request to the *web-api* by running the following commands:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完这两个新的服务文件后，请运行它们，然后通过运行以下命令生成对 *web-api* 的请求：
- en: '[PRE19]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: A new field, named `trace`, should now be present in the output of the `curl`
    command. This is the trace ID for the series of requests that have been passed
    between the services. The value should be 16 hexadecimal characters, and in my
    case, I received the value `e232bb26a7941aab`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '`curl` 命令的输出中现在应该有一个名为 `trace` 的新字段。这是传递给服务之间请求系列的追踪 ID。该值应为 16 个十六进制字符，在我的情况下，我收到的值是
    `e232bb26a7941aab`。'
- en: Visualizing a Request Tree
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化请求树
- en: 'Data about the requests have been sent to your Zipkin server instance. It’s
    now time to open the web interface and see how that data is visualized. Open the
    following URL in your browser:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请求数据已发送到你的 Zipkin 服务器实例。现在是时候打开网页界面，看看数据如何可视化了。在浏览器中打开以下 URL：
- en: '[PRE20]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You should now be greeted with the Zipkin web interface. It’s not too exciting
    just yet. The left sidebar contains two links. The first one, which looks like
    a magnifying glass, is to the current Discover screen. The second link, resembling
    network nodes, links to the Dependencies screen. At the top of the screen is a
    plus sign, which can be used for specifying which requests to search for. With
    this tool you can specify criteria like the service name or tags. But for now
    you can ignore those. In the upper-right corner is a simple search button, one
    that will display recent requests. Click the magnifying glass icon, which will
    perform the search.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能看到 Zipkin 网页界面。它目前可能不太激动人心。左侧边栏包含两个链接。第一个像放大镜一样，是当前的发现屏幕。第二个链接类似网络节点，指向依赖关系屏幕。屏幕顶部有一个加号，用于指定要搜索的请求。使用此工具，你可以指定诸如服务名称或标签等条件。但现在你可以忽略这些。屏幕右上角有一个简单的搜索按钮，点击放大镜图标即可执行搜索。
- en: '[Figure 4-8](#fig_zipkin_discover) is an example of what the interface should
    look like after you’ve performed a search. Assuming you ran the `curl` command
    just once, you should see only a single entry.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-8](#fig_zipkin_discover) 展示了在执行搜索后界面应该看起来的样子。假设你只运行了一次 `curl` 命令，你应该只能看到一个条目。'
- en: '![Screenshot of a Zipkin Discover page](assets/dsnj_0408.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![Zipkin Discover 页面的截图](assets/dsnj_0408.png)'
- en: Figure 4-8\. Zipkin discover interface
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. Zipkin discover 界面
- en: Click the entry to be taken to the timeline view page. This page displays content
    in two columns. The column on the left displays a timeline of requests. The horizontal
    axis represents time. The units on the top of the timeline display how much time
    has passed since the very first `SERVER` trace was made with the given trace ID.
    The vertical rows represent the depth of the request; as each subsequent service
    makes another request, a new row will be added.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 单击条目以进入时间轴视图页面。此页面以两列显示内容。左列显示请求的时间轴。水平轴表示时间。时间轴顶部的单位显示自第一个具有给定跟踪ID的`SERVER`跟踪开始以来经过的时间。垂直行表示请求的深度；随着每个后续服务发出另一个请求，将添加新行。
- en: For your timeline, you should see two rows. The first row was generated by the
    *web-api* and has a call named *get_root*. The second row was generated by the
    *recipe-api* and has a call named *get_recipe*. A more complex version of the
    timeline you’re seeing, based on the previously mentioned system with an additional
    *user-api* and *user-store*, is displayed in [Figure 4-9](#fig_zipkin_timeline).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的时间轴，您应该看到两行。第一行由*web-api*生成，并有一个名为*get_root*的调用。第二行由*recipe-api*生成，并有一个名为*get_recipe*的调用。基于先前提到的具有额外的*user-api*和*user-store*的系统的更复杂版本的时间轴，显示在[图4-9](#fig_zipkin_timeline)中。
- en: '![Screenshot of a Zipkin Trace timeline](assets/dsnj_0409.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![Zipkin跟踪时间轴的屏幕截图](assets/dsnj_0409.png)'
- en: Figure 4-9\. Example Zipkin trace timeline
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-9\. 示例Zipkin跟踪时间轴
- en: Click the second row. The right column will be updated to display additional
    metadata about the request. The Annotations bar displays a timeline for the span
    you clicked. Depending on the speed of the request, you will see between two and
    four dots. The furthest left and furthest right dots represent the time that the
    client perceived the request to take. If the request was slow enough, you should
    see two inner dots, and those will represent the time the server perceived the
    request to take. Since these services are so fast, the dots might overlap and
    will be hidden by the Zipkin interface.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 单击第二行。右列将更新以显示有关请求的附加元数据。注解栏显示了您点击的跨度的时间轴。根据请求的速度，您将看到两到四个点。最左侧和最右侧的点表示客户端感知请求所需的时间。如果请求足够慢，您应该会看到两个内部点，这些点表示服务器感知请求所需的时间。由于这些服务速度很快，点可能重叠，并且会被Zipkin界面隐藏。
- en: The Tags section displays the tags associated with the request. This can be
    used to debug which endpoints are taking the longest time to process and which
    service instances (by using the IP address and port) are to blame.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 标签部分显示与请求关联的标签。这可用于调试哪些端点需要最长时间来处理，以及哪些服务实例（通过使用IP地址和端口）是罪魁祸首。
- en: Visualizing Microservice Dependencies
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化微服务依赖关系
- en: The Zipkin interface can also be used to show aggregate information about the
    requests that it receives. Click the Dependencies link in the sidebar to be taken
    to the dependencies screen. The screen should be mostly blank, with a selector
    at the top to specify a time range and perform a search. The default values should
    be fine, so click the magnifying glass icon to perform a search.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Zipkin界面还可以用来显示其接收到的请求的聚合信息。单击侧边栏中的依赖项链接以进入依赖项屏幕。屏幕应该大部分是空白的，顶部有一个选择器用于指定时间范围并执行搜索。默认值应该是合适的，因此单击放大镜图标执行搜索。
- en: The screen will then be updated to display two nodes. Zipkin has searched through
    the different spans it found that matched the time range. Using this information,
    it has determined how the services are related to each other. With the two example
    applications, the interface isn’t all that interesting. On the left, you should
    see a node representing the *web-api* (where requests originate), and on the right,
    you should see a node representing the *recipe-api* (the deepest service in the
    stack). Small dots move from the left of the screen to the right, showing the
    relative amount of traffic between the two nodes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕随即更新，显示出两个节点。Zipkin已经搜索了与时间范围匹配的不同跨度。利用这些信息，它已确定了服务之间的关联关系。对于这两个示例应用程序，界面并不是那么有趣。在左侧，您应该看到表示*web-api*（请求发起的地方）的节点，而在右侧，您应该看到表示*recipe-api*（堆栈中最深的服务）的节点。小点从屏幕左侧移动到右侧，显示两个节点之间流量的相对量。
- en: If you were using Zipkin with many different services within an organization,
    you would see a much more complex map of the relationships between services. [Figure 4-10](#fig_zipkin_dependencies)
    is an example of what the relationships between the four services in the more
    complex example would look like.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在组织中使用 Zipkin 与许多不同的服务，您将看到服务之间关系更复杂的地图。[图 4-10](#fig_zipkin_dependencies)
    是更复杂示例中四个服务之间关系的一个例子。
- en: '![Pseudo screenshot of a Zipkin Dependencies view](assets/dsnj_0410.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![Zipkin Dependencies 视图的伪截图](assets/dsnj_0410.png)'
- en: Figure 4-10\. Example Zipkin dependency view
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. Zipkin 依赖视图示例
- en: Assuming every service within an organization uses Zipkin, such a diagram would
    be a very powerful tool for understanding the interconnections between services.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 假设组织中的每个服务都使用 Zipkin，这样的图表将是理解服务之间相互连接的强大工具。
- en: Health Checks
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: '[“Load Balancing and Health Checks”](ch03.html#ch_scaling_sec_rp_subsec_health)
    looked at how HAProxy can be configured to automatically remove and re-add a running
    service instance to the pool of candidate instances for routing requests to. HAProxy
    can do this by making an HTTP request to an endpoint of your choosing and checking
    the status code. Such an endpoint is also useful for checking the *liveness* of
    a service—which is a term meaning a newly deployed service has finished the startup
    stage and is ready to receive requests (like establishing a database connection).
    Kubernetes, which is covered in [Chapter 7](ch07.html#ch_kubernetes), can also
    make use of such a liveness check. It is generally useful for an application to
    know if it’s healthy or not.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '["负载均衡和健康检查"](ch03.html#ch_scaling_sec_rp_subsec_health) 讲述了如何配置 HAProxy 自动将运行中的服务实例从候选实例池中移除并重新添加，以便路由请求。HAProxy
    可以通过向您选择的端点发出 HTTP 请求并检查状态码来实现此功能。这样的端点也对检查服务的*liveness*（即新部署的服务已完成启动阶段，并准备接收请求，比如建立数据库连接）非常有用。Kubernetes
    在[第 7 章](ch07.html#ch_kubernetes)中也可以利用这样的 liveness 检查。了解应用程序是否健康通常是非常有用的。'
- en: An application can usually be considered healthy if it is able to respond to
    incoming requests with correct data without ill side effects. The specifics of
    how to measure this will change depending on the application. If an application
    needs to make a connection to a database, and such a connection is lost, then
    the application probably won’t be able to process the requests it receives. (Note
    that your application should attempt to reconnect to databases; this is covered
    in [“Database Connection Resilience”](ch08.html#ch_resilience_sec_db).) In such
    a case, it would make sense to have the application declare itself unhealthy.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，应用程序如果能够正确响应请求并且没有不良副作用，就可以被视为健康。如何衡量这一点的具体方法会根据应用程序的不同而变化。如果一个应用程序需要连接数据库，而连接丢失，那么该应用程序可能无法处理接收到的请求。（请注意，您的应用程序应该尝试重新连接数据库；这在["数据库连接弹性"](ch08.html#ch_resilience_sec_db)中有所涵盖。）在这种情况下，应用程序声明自身不健康是有意义的。
- en: On the other hand, some features are a bit of a grey area. For example, if a
    service is unable to establish a connection to a caching service but is still
    able to connect to a database and serve requests, it is probably fine to declare
    itself healthy. The grey area in this case is with response time. If the service
    is no longer able to achieve its SLA, then it might be dangerous to run because
    it could cost your organization money. In this situation, it might make sense
    to declare the service *degraded*.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，有些特性存在一些灰色地带。例如，如果一个服务无法建立到缓存服务的连接，但仍能连接到数据库并处理请求，那么可能可以宣布自己健康。在这种情况下的灰色地带是响应时间。如果服务无法再实现其
    SLA，那么继续运行可能会成为风险，因为这可能会给您的组织带来损失。在这种情况下，宣布服务*降级*可能是有道理的。
- en: What would happen in this situation if the degraded service were to declare
    itself unhealthy? The service might be restarted by some sort of deployment management
    tool. However, if the problem is that the caching service is down, then perhaps
    every single service would be restarted. This can lead to situations where no
    service is available to serve requests. This scenario will be covered in [“Alerting
    with Cabot”](#ch_monitoring_sec_alert). For now, consider slow/degraded services
    healthy.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果降级服务宣布自己不健康，会发生什么情况？某种部署管理工具可能会重新启动服务。但是，如果问题是缓存服务停机，那么可能会重新启动每个服务。这可能导致没有服务可用来提供请求。此场景将在
    [“使用 Cabot 进行警报”](#ch_monitoring_sec_alert) 中进行覆盖。目前，认为缓慢/降级的服务是健康的。
- en: Health checks are usually run periodically. Sometimes they are triggered by
    a request from an external service, such as HAProxy making an HTTP request (an
    operation that defaults to every two seconds). Sometimes they are triggered internally,
    such as a `setInterval()` call that checks the application’s health before reporting
    to an external discovery service like *Consul* that it is healthy (a check that
    runs perhaps every 10 seconds). In any case, the overhead of running the health
    check should not be so high that the process is slowed down or the database is
    overwhelmed.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 健康检查通常定期运行。有时会由外部服务的请求触发，例如 HAProxy 发起的 HTTP 请求（默认每两秒执行一次）。有时会在内部触发，例如通过 `setInterval()`
    调用，在向外部发现服务（如 *Consul*）报告健康状态之前检查应用程序的健康状况（可能每 10 秒运行一次）。无论如何，运行健康检查的开销都不应太高，以至于进程被减速或数据库被压垮。
- en: Building a Health Check
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建健康检查
- en: In this section you will build a health check for a rather boring service. This
    application will have both a connection to a Postgres database, resembling a persistent
    data store, as well as a connection to Redis, which will represent a cache.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将为一个相当无聊的服务构建健康检查。此应用程序将同时连接到类似于持久数据存储的 PostgreSQL 数据库，以及代表缓存的 Redis 连接。
- en: Before you start writing code, you’ll need to run the two backing services.
    Run the commands in [Example 4-11](#ex_postgres_redis) to get a copy of Postgres
    and Redis running. You’ll need to run each command in a new terminal window. Ctrl
    + C can be used to kill either service.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写代码之前，您需要运行两个后端服务。运行 [Example 4-11](#ex_postgres_redis) 中的命令来启动 Postgres
    和 Redis 的副本。您需要在新的终端窗口中运行每个命令。可以使用 Ctrl + C 来终止任何一个服务。
- en: Example 4-11\. Running Postgres and Redis
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-11\. 运行 Postgres 和 Redis
- en: '[PRE21]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Next, create a new file from scratch named *basic-http-healthcheck.js*. Insert
    the content from [Example 4-12](#ex_health_check) into your newly created file.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，从头开始创建一个名为 *basic-http-healthcheck.js* 的新文件。将内容从 [Example 4-12](#ex_health_check)
    插入到您新创建的文件中。
- en: Example 4-12\. *basic-http-healthcheck.js*
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-12\. *basic-http-healthcheck.js*
- en: '[PRE22]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](assets/1.png)](#co_observability_CO6-1)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO6-1)'
- en: Redis requests will fail when offline.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 离线时请求将失败。
- en: '[![2](assets/2.png)](#co_observability_CO6-2)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO6-2)'
- en: Completely fail if Postgres cannot be reached.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法访问 Postgres，则完全失败。
- en: '[![3](assets/3.png)](#co_observability_CO6-3)'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO6-3)'
- en: Pass with a degraded state if Redis cannot be reached.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果无法访问 Redis，则在降级状态下通过。
- en: This file makes use of the `ioredis` package for connecting to and issuing queries
    for Redis. It also makes use of the `pg` package for working with Postgres. When
    `ioredis` is instantiated it will default to connecting to a locally running service,
    which is why connection details aren’t necessary. The `enableOfflineQueue` flag
    specifies if commands should be queued up when the Node.js process can’t connect
    to the Redis instance. It defaults to `true`, meaning requests can be queued up.
    Since Redis is being used as a caching service—not as a primary data store—the
    flag should set to `false`. Otherwise, a queued-up request to access the cache
    could be slower than connecting to the real data store.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件利用了 `ioredis` 包来连接和查询 Redis。它还使用了 `pg` 包来处理 PostgreSQL。当实例化 `ioredis` 时，它会默认连接到本地运行的服务，这就是为什么不需要连接详细信息。`enableOfflineQueue`
    标志指定当 Node.js 进程无法连接到 Redis 实例时是否应将命令排队。它默认为 `true`，意味着请求可以排队。由于 Redis 被用作缓存服务而不是主要数据存储，所以该标志应设置为
    `false`。否则，排队访问缓存的请求可能比连接到真实数据存储要慢。
- en: The `pg` package also defaults to connecting to a Postgres instance running
    locally, but it will still need some connection information. That will be provided
    using environment variables.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`pg` 包默认连接到本地运行的 Postgres 实例，但仍然需要一些连接信息。这些信息将使用环境变量提供。'
- en: This health check endpoint is configured to first check for features that are
    critical to run. If any of those features are lacking, then the endpoint will
    immediately fail. In this case, only the Postgres check applies, but a real application
    might have more. After that, the checks that will result in a degraded service
    are run. Only the Redis check applies in this situation. Both of these checks
    work by querying the backing store and checking for a sane response.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 此健康检查端点首先配置为检查运行所需的关键特性。如果其中任何特性缺失，端点将立即失败。在本例中，仅适用于 Postgres 检查，但真实应用程序可能会有更多检查。随后运行会导致服务降级的检查。在此情况下，仅适用于
    Redis 检查。这些检查通过查询后端存储并检查其合理响应来工作。
- en: Note that a degraded service will return a 200 status code. HAProxy could, for
    example, be configured to still direct requests to this service. If the service
    is degraded, then an alert could be generated (see [“Alerting with Cabot”](#ch_monitoring_sec_alert)).
    Figuring out *why* the cache isn’t working is something that our application shouldn’t
    be concerned about. The issue might be that Redis itself has crashed or that there
    is a network issue.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，降级的服务将返回 200 状态码。例如，HAProxy 可以配置为仍然将请求定向到此服务。如果服务降级，则可能会生成警报（参见[“使用 Cabot
    进行警报”](#ch_monitoring_sec_alert)）。弄清楚*为什么*缓存不起作用不是我们应用程序应该关心的事情。问题可能是 Redis 本身崩溃了或存在网络问题。
- en: 'Now that the service file is ready, run the following command to start the
    service:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务文件准备就绪，请运行以下命令启动服务：
- en: '[PRE23]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The Postgres connection variables have been provided as environment variables
    and are used by the underlying `pg` package. Explicitly naming the variables in
    code is a better approach for production code, and these variables are only used
    for brevity.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 已提供了作为环境变量的 Postgres 连接变量，并由底层 `pg` 包使用。在生产代码中，显式命名这些变量是更好的方法，这些变量仅用于简洁性。
- en: Now that your service is running, it’s time to try using the health checks.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的服务正在运行，现在是时候尝试使用健康检查了。
- en: Testing the Health Check
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试健康检查
- en: 'With the process running and connecting to the databases, it should be considered
    in a healthy state. Issue the following request to check the status of the application:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 进程运行并连接到数据库后，应考虑其是否处于健康状态。执行以下请求以检查应用程序的状态：
- en: '[PRE24]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The response should contain the message `OK` and have an associated 200 status
    code.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 响应应包含消息 `OK` 并具有相关的 200 状态码。
- en: Now we can simulate a degraded situation. Switch focus to the Redis service
    and press Ctrl + C to kill the process. You should see some error messages printed
    from the Node.js process. They will start off quickly and then slow down as the
    `ioredis` module uses *exponential backoff* when attempting to reconnect to the
    Redis server. This means that it retries rapidly and then slows down.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以模拟降级情况。将注意力转向 Redis 服务，并按下 Ctrl + C 杀死进程。您将看到一些错误消息从 Node.js 进程中打印出来。它们将快速开始，然后由于
    `ioredis` 模块在尝试重新连接到 Redis 服务器时使用*指数退避*而变慢。这意味着它会快速重试，然后减慢速度。
- en: Now that the application is no longer connected to Redis, run the same `curl`
    command again. This time, the response body should contain the message `DEGRADED`,
    though it will still have a 200 status code.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应用程序不再连接到 Redis，再次运行相同的 `curl` 命令。这次，响应体应包含消息 `DEGRADED`，尽管仍然具有 200 状态码。
- en: Switch back to the terminal window you previously ran Redis with. Start the
    Redis service again, switch back to the terminal where you ran `curl`, and run
    the request again. Depending on your timing, you might still receive the `DEGRADED`
    message, but you will eventually get the `OK` message once `ioredis` is able to
    reestablish a connection.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回您之前运行 Redis 的终端窗口。重新启动 Redis 服务，切换回您运行 `curl` 的终端，并再次运行请求。根据您的时间安排，您可能仍会收到
    `DEGRADED` 消息，但一旦 `ioredis` 能够重新建立连接，最终将收到 `OK` 消息。
- en: Note that killing Postgres in this manner will cause the application to crash.
    The `pg` library doesn’t provide the same automatic reconnection feature that
    `ioredis` provides. Additional reconnection logic will need to be added to the
    application to get that working. [“Database Connection Resilience”](ch08.html#ch_resilience_sec_db)
    contains an example of this.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，以这种方式终止Postgres会导致应用程序崩溃。`pg`库不像`ioredis`提供相同的自动重连功能。需要向应用程序添加额外的重连逻辑来使其正常工作。[“数据库连接韧性”](ch08.html#ch_resilience_sec_db)中包含了一个示例。
- en: Alerting with Cabot
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Cabot进行警报
- en: There are certain issues that simply cannot be resolved by automatically killing
    and restarting a process. Issues related to stateful services, like the downed
    Redis service mentioned in the previous section, are an example. Elevated 5XX
    error rates are another common example. In these situations it’s often necessary
    to alert a developer to find the root cause of an issue and correct it. If such
    errors can cause a loss of revenue, then it becomes necessary to wake developers
    up in the middle of the night.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 有些问题简单地通过自动终止和重启进程来解决是不可能的。与前一节提到的状态服务相关的问题，如下线的Redis服务，就是一个例子。升高的5XX错误率是另一个常见的例子。在这些情况下，通常需要提醒开发者找出问题的根本原因并加以修复。如果这类错误会导致收入损失，那么半夜唤醒开发者就成为必要。
- en: In these situations a cellphone is usually the best medium for waking a developer,
    often by triggering an actual phone call. Other message formats, such as emails,
    chat room messages, and text messages, usually aren’t accompanied by an annoying
    ringing sound and often won’t suffice for alerting the developer.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，手机通常是唤醒开发者的最佳媒介，通常通过触发实际的电话来实现。其他消息格式，如电子邮件、聊天室消息和短信，通常没有令人烦恼的响铃声，往往无法满足开发者的警报需求。
- en: In this section, you’ll set up an instance of [*Cabot*](https://cabotapp.com),
    which is an open source tool for polling the health of an application and triggering
    alerts. Cabot supports multiple forms of health checks, such as querying Graphite
    and comparing reported values to a threshold, as well as pinging a host. Cabot
    also supports making an HTTP request, which is what is covered in this section.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你将设置一个[*Cabot*](https://cabotapp.com)实例，这是一个用于轮询应用程序健康状态和触发警报的开源工具。Cabot支持多种健康检查形式，例如查询Graphite并将报告的值与阈值进行比较，以及ping主机。Cabot还支持发出HTTP请求，这正是本节所涵盖的内容。
- en: In this section, you’ll also create a free *Twilio* trial account. Cabot can
    use this account to both send SMS messages and make phone calls. You can skip
    this part if you would prefer not to create a Twilio account. In that case, you’ll
    just see a dashboard changing colors from a happy green to an angry red.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，你还将创建一个免费的*Twilio*试用账户。Cabot可以使用此账户发送短信和打电话。如果你不想创建Twilio账户，可以跳过这一部分。在那种情况下，你将只能看到一个仪表板从愉快的绿色变成愤怒的红色。
- en: The examples in this section will have you create a single user in Cabot, and
    that user will receive all the alerts. In practice, an organization will set up
    schedules, usually referred to as the on-call rotation. In these situations, the
    person who will receive an alert will depend on the schedule. For example, the
    person on call might be Alice on call week one, Bob on week two, Carol on week
    three, and back to Alice on week four.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的示例将让你在Cabot中创建一个单一用户，并且该用户将接收所有警报。实际情况中，组织会设置排班计划，通常称为值班轮换。在这些情况下，接收警报的人员将依赖于排班。例如，第一周可能是Alice，第二周是Bob，第三周是Carol，然后又回到Alice。
- en: Note
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Another important feature in a real organization is something called a *runbook*.
    A runbook is usually a page in a wiki and is associated with a given alert. The
    runbook contains information on how to diagnose and fix an issue. That way, when
    an engineer gets a notification at 2 A.M. about the *Database Latency* alert,
    they can read about how to access the database and run a query. You won’t create
    a runbook for this example, but you must be diligent in doing so for real-world
    alerts.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实组织中的另一个重要特性是所谓的*运行手册*。运行手册通常是wiki中的一页，与特定警报相关联。运行手册包含如何诊断和修复问题的信息。这样，当工程师在凌晨2点收到*数据库延迟*警报时，他们可以了解如何访问数据库并运行查询。对于这个示例，你不需要创建运行手册，但在实际情况下务必做到如此。
- en: Create a Twilio Trial Account
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Twilio试用账户
- en: At this point, head over to [*https://twilio.com*](https://twilio.com) and create
    a trial account. When you create an account, you will get two pieces of data that
    you will need for configuring Cabot. The first piece of information is called
    an *Account SID*. This is a string that starts with `AC` and contains a bunch
    of hexadecimal characters. The second piece of information is the *Auth Token*.
    This value just looks like normal hexadecimal characters.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，请前往[*https://twilio.com*](https://twilio.com)，并创建一个试用账户。创建账户时，你将获得两个数据，这些数据是配置Cabot所需的。第一条信息称为*Account
    SID*。这是一个以`AC`开头并包含一串十六进制字符的字符串。第二条信息是*Auth Token*。这个值看起来只是普通的十六进制字符。
- en: Using the interface, you’ll also need to configure a *Trial Number*. This is
    a virtual phone number that you can use with this project. The phone number begins
    with a plus sign followed by a country code and the rest of the number. You’ll
    need to use this number within your project, including the plus sign and country
    code. The number you receive might look like *+15551234567*.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 使用界面时，你还需要配置一个*Trial Number*。这是一个虚拟电话号码，你可以在这个项目中使用。电话号码以加号和国家代码开头，后面跟着号码的其余部分。你需要在项目中使用这个号码，包括加号和国家代码。你收到的号码可能看起来像*+15551234567*。
- en: Finally, you’ll need to configure your personal cellphone’s phone number to
    be a *Verified Number*/*Verified Caller ID* in Twilio. This allows you to confirm
    with Twilio that the phone number you have belongs to you and that you’re not
    just using Twilio to send spam texts to strangers, a process that is a limitation
    of the Twilio trial account. After you verify your phone number, you’ll be able
    to configure Cabot to send an SMS message to it.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要将你个人手机的电话号码配置为Twilio的*Verified Number*/*Verified Caller ID*。这样你就可以确认该电话号码属于你，而你并不只是利用Twilio向陌生人发送垃圾短信。这是Twilio试用账户的一项限制。验证你的电话号码后，你可以配置Cabot向其发送短信。
- en: Running Cabot via Docker
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过Docker运行Cabot
- en: 'Cabot is a little more complex than the other services covered in this chapter.
    It requires several Docker images, not just a single one. For that reason you’ll
    need to use *Docker Compose* to launch several containers, instead of launching
    a single one using Docker. Run the following commands to pull the git repository
    and check out a commit that is known to be compatible with this example:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: Cabot比本章涵盖的其他服务更复杂一些。它需要多个Docker镜像，而不仅仅是一个单一的镜像。因此，你需要使用*Docker Compose*启动多个容器，而不是使用Docker启动单个容器。运行以下命令拉取git仓库，并检出一个已知与此示例兼容的提交：
- en: '[PRE25]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Next, create a new file located at *conf/production.env* within this repository.
    Note that it’s not within the *distributed-node* directory that you’ve been creating
    all your other project files in. Add the content from [Example 4-13](#ex_cabot_config)
    to this file.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在这个仓库中创建一个位于*conf/production.env*的新文件。请注意，这并不是你在*distributed-node*目录中创建的其他项目文件。将[示例 4-13](#ex_cabot_config)中的内容添加到这个文件中。
- en: Example 4-13\. *config/production.env*
  id: totrans-345
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. *config/production.env*
- en: '[PRE26]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](assets/1.png)](#co_observability_CO7-1)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observability_CO7-1)'
- en: Set this value to your [TZ Time Zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 将此值设置为你的[TZ时区](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)。
- en: '[![2](assets/2.png)](#co_observability_CO7-2)'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observability_CO7-2)'
- en: For extra credit, configure a Graphite source using your IP address.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了额外的学分，配置一个使用你的IP地址的Graphite源。
- en: '[![3](assets/3.png)](#co_observability_CO7-3)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observability_CO7-3)'
- en: Omit these lines if you’re not using Twilio. Be sure to prefix the phone number
    with a plus sign and country code.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不使用Twilio，请省略这些行。请确保在电话号码前加上加号和国家代码。
- en: Tip
  id: totrans-353
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you’re feeling adventurous, configure the `GRAPHITE_API` line to use the
    same Graphite instance that you created in [“Metrics with Graphite, StatsD, and
    Grafana”](#ch_monitoring_sec_metrics). Later, when using the Cabot interface,
    you can choose which metrics to create an alert on. This is useful for taking
    a metric, like request timing, and alerting once it surpasses a certain threshold,
    such as 200ms. However, for brevity, this section won’t cover how to set it up,
    and you can omit the line.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意尝试，配置`GRAPHITE_API`行以使用你在[“使用Graphite、StatsD和Grafana进行指标监控”](#ch_monitoring_sec_metrics)中创建的相同的Graphite实例。稍后，在使用Cabot界面时，你可以选择对哪些指标创建警报。这对于获取指标（如请求时间）并在其超过一定阈值（如200毫秒）时发出警报非常有用。但为简洁起见，本节不涵盖设置步骤，你可以省略该行。
- en: 'Once you’ve finished configuring Cabot, run the following command to start
    the Cabot service:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Cabot 完成后，运行以下命令启动 Cabot 服务：
- en: '[PRE27]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This will cause several Docker containers to start running. In the terminal,
    you should see progress as each image is downloaded, followed by colored output
    associated with each container once it’s running. Once things have settled down,
    you’re ready to move on to the next step.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这将导致几个 Docker 容器开始运行。在终端中，您会看到每个镜像下载完成后的进度，以及每个容器运行时的彩色输出。一切安定下来后，您就可以继续下一步了。
- en: Creating a Health Check
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建健康检查
- en: For this example, use the same *basic-http-healthcheck.js* file from [Example 4-12](#ex_health_check)
    that you made in the previous section. Execute that file and run the Postgres
    service as configured in [Example 4-11](#ex_postgres_redis). Once that is done,
    Cabot can be configured to make use of the */health* endpoint the Node.js service
    exposes.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例，使用您在前一节中创建的 *basic-http-healthcheck.js* 文件（[示例 4-12](#ex_health_check)）。执行该文件，并按照
    [示例 4-11](#ex_postgres_redis) 中配置的方式运行 Postgres 服务。完成后，Cabot 可以配置为使用 Node.js 服务公开的
    */health* 端点。
- en: With the Node.js service now running, open the Cabot web service using your
    web browser by visiting [*http://localhost:5000*](http://localhost:5000).
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Node.js 服务正在运行，请使用您的 Web 浏览器打开 Cabot Web 服务，访问 [*http://localhost:5000*](http://localhost:5000)。
- en: You’ll first be prompted to create an administrative account. Use the default
    username `admin`. Next, put in your email address and a password and click Create.
    Then, you’ll be prompted to log in. Type `admin` for the username field, enter
    your password again, then click Log in. You’ll finally be taken to the services
    screen that will contain no entries.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您将被提示创建一个管理员账户。使用默认用户名 `admin`。接下来，输入您的电子邮件地址和密码，然后点击创建。然后，您将被提示登录。在用户名字段输入
    `admin`，再次输入密码，然后点击登录。最后，您将进入不包含任何条目的服务屏幕。
- en: On the empty services screen, click the large plus symbol to be taken to the
    [New service](http://localhost:5000/service/create/) screen. Then, input the information
    from [Table 4-3](#table_cabot_create_service) into the create service form.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在空的服务屏幕上，点击大加号符号，以跳转到 [新服务](http://localhost:5000/service/create/) 屏幕。然后，将 [表 4-3](#table_cabot_create_service)
    中的信息输入到创建服务表单中。
- en: Table 4-3\. Fields for creating a service in Cabot
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. 在 Cabot 中创建服务的字段
- en: '| Name | Dist Node Service |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | Dist Node Service |'
- en: '| Url | http://<LOCAL_IP>:3300/ |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 网址 | http://<LOCAL_IP>:3300/ |'
- en: '| Users to notify | admin |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 需通知的用户 | admin |'
- en: '| Alerts | Twilio SMS |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 警报 | Twilio 短信 |'
- en: '| Alerts enabled | checked |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 启用警报 | 已选中 |'
- en: Again, you’ll need to replace `<LOCAL_IP>` with your IP address. Once you’ve
    entered the information, click the Submit button. This will take you to a screen
    where you can view the [Dist Node Service](http://localhost:5000/service/1/) overview.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您需要将 `<LOCAL_IP>` 替换为您的 IP 地址。输入信息后，点击提交按钮。这将带您到一个屏幕，您可以在那里查看 [Dist Node
    Service](http://localhost:5000/service/1/) 的概述。
- en: On this screen, scroll down to the Http checks section and click the plus sign
    to be taken to the [New check](https://oreil.ly/voFxA) screen. On this screen,
    input the information from [Table 4-4](#table_cabot_create_check) into the “create
    check” form.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在此屏幕上，向下滚动至 Http 检查部分，并点击加号图标，以跳转到 [新检查](https://oreil.ly/voFxA) 屏幕。在此屏幕上，将
    [表 4-4](#table_cabot_create_check) 中的信息输入到“创建检查”表单中。
- en: Table 4-4\. Fields for creating an HTTP check in Cabot
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-4\. 在 Cabot 中创建 HTTP 检查的字段
- en: '| Name | Dist Node HTTP Health |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | Dist Node HTTP Health |'
- en: '| Endpoint | http://<LOCAL_IP>:3300/health |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 终端 | http://<LOCAL_IP>:3300/health |'
- en: '| Status code | 200 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 状态码 | 200 |'
- en: '| Importance | Critical |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 重要性 | 关键 |'
- en: '| Active | checked |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 活跃 | 已选中 |'
- en: '| Service set | Dist Node Service |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 服务集 | Dist Node Service |'
- en: Once you’ve entered that information, click the Submit button. This will take
    you back to the [Dist Node Service](http://localhost:5000/service/1/) overview
    screen.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 输入信息后，点击提交按钮。这将带您回到 [Dist Node Service](http://localhost:5000/service/1/) 的概述屏幕。
- en: Next, the `admin` account needs to be configured to receive alerts using Twilio
    SMS. In the upper-right corner of the screen, click the admin drop-down menu,
    then click Profile settings. On the left sidebar, click the Twilio Plugin link.
    This form will ask you for your phone number. Enter your phone number, beginning
    with a plus symbol and the country code. This number should match the verified
    number that you previously entered in your Twilio account. Once you’re done, click
    the Submit button.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要配置`admin`账户以使用 Twilio SMS 接收警报。在屏幕右上角，点击管理员下拉菜单，然后点击“Profile settings”。在左侧边栏，点击“Twilio
    Plugin”链接。这个表单会要求你输入你的电话号码。输入你的电话号码，以加号和国家代码开头。这个号码应该与你之前在 Twilio 账户中验证的号码匹配。完成后，点击“Submit”按钮。
- en: Once you’re done setting your phone number, click the Checks link in the top
    navigation bar. This will take you to the [Checks](http://localhost:5000/check/1/)
    listing page, which should contain the one entry you’ve created. Click the single
    entry, [Dist Node HTTP Health](http://localhost:5000/check/1/), to be taken to
    the health check history listing. At this point, you should only see one or two
    entries since they run once every five minutes. These entries should have a green
    “succeeded” label next to them. Click the circular arrow icon in the upper right
    to trigger another health check.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 设置完手机号码后，点击顶部导航栏中的“Checks”链接。这将带你到 [Checks](http://localhost:5000/check/1/)
    列表页面，页面上应该只包含你创建的一个条目。点击单个条目，[Dist Node HTTP Health](http://localhost:5000/check/1/)，将带你到健康检查历史列表。此时，你应该只能看到一两个条目，因为它们每五分钟运行一次。这些条目旁边应该有一个绿色的“成功”标签。点击右上角的圆形箭头图标以触发另一个健康检查。
- en: Now switch back to the terminal window where your Node.js service is running.
    Kill it with Ctrl + C. Then, switch back to Cabot and click the icon to run the
    test again. This time the test will fail, and you’ll get a new entry in the list
    with a red background and the word “failed.”
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 现在切换回你的 Node.js 服务正在运行的终端窗口。使用 Ctrl + C 终止它。然后切换回 Cabot 并点击图标再次运行测试。这次测试将失败，并且你将在列表中看到一个新的带有红色背景和“失败”字样的条目。
- en: 'You should also get a text message containing information about the alert.
    The message I received is shown here:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该收到一条包含警报信息的文本消息。我收到的消息显示在这里：
- en: '[PRE28]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If Cabot were properly installed on a real server somewhere with a real hostname,
    the text message would contain a working link that could then be opened on your
    phone. However, since Cabot is probably running on your laptop, the URL doesn’t
    make a lot of sense in this context.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Cabot 在某个真实服务器上正确安装并且有真实的主机名，文本消息将包含一个可以在你的手机上打开的有效链接。然而，由于 Cabot 可能在你的笔记本电脑上运行，这个
    URL 在这种情况下就没什么意义了。
- en: Click the Services link at the top of the screen, then click the Dist Node Service
    link again. On this screen, you’ll now see a graph displaying the status of the
    service, as well as a banner stating that the service is critical, like in [Figure 4-11](#fig_cabot_status).
    Now click the Acknowledge alert button to pause the alerts for 20 minutes. This
    is useful for giving you time to work on the issue without being alerted over
    and over. It’s now time to fix the failing service.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 点击屏幕顶部的“Services”链接，然后再次点击“Dist Node Service”链接。在这个屏幕上，你现在会看到显示服务状态的图表，以及一个标语，表明服务是关键的，就像在[图 4-11](#fig_cabot_status)中一样。现在点击“Acknowledge
    alert”按钮以暂停 20 分钟的警报。这对于给你解决问题的时间而不断地被警报通知是非常有用的。现在是时候修复失败的服务了。
- en: '![A screenshot of Cabot showing the failing Dist Node Service, with a warning
    banner and an acknowledge button](assets/dsnj_0411.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![一个 Cabot 的截图，显示了失败的 Dist Node 服务，带有警告横幅和确认按钮](assets/dsnj_0411.png)'
- en: Figure 4-11\. Cabot service status screenshot
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-11\. Cabot 服务状态截图
- en: Switch back to the terminal where you ran the Node.js process and start it again.
    Then, switch back to the browser. Navigate back to the HTTP check you created.
    Click the icon to trigger the check again. This time the check should succeed,
    and it will switch back to a green “succeeded” message.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 切换回你运行 Node.js 进程的终端，然后再次启动它。然后切换回浏览器。导航回你创建的 HTTP 检查。点击图标再次触发检查。这次检查应该成功，并且将切换回一个绿色的“成功”消息。
- en: Cabot, as well as other alerting tools, offers the ability to assign different
    users to different services. This is important since different teams within an
    organization will own different services. When you created an HTTP alert, it was
    also possible to provide a regex to be applied against the body. This can be used
    to differentiate a degraded service from an unhealthy service. Cabot can then
    be configured to have an unhealthy service alert an engineer but have a degraded
    service merely be highlighted in the UI.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Cabot以及其他警报工具提供了将不同用户分配给不同服务的功能。这一点非常重要，因为组织内的不同团队将拥有不同的服务。当您创建HTTP警报时，也可以提供一个正则表达式来应用于正文。这可以用来区分降级服务和不健康服务。然后可以配置Cabot，使不健康的服务警报工程师，而仅在UI中突出显示降级服务。
- en: 'At this point you’re done with the Cabot Docker containers. Switch to the window
    where you were running Cabot and press Ctrl + C to kill it. Then run the following
    command to remove the containers from your system:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已完成Cabot Docker容器的操作。切换到运行Cabot的窗口，按Ctrl + C以终止它。然后运行以下命令从系统中删除这些容器：
- en: '[PRE29]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ^([1](ch04.html#idm46291188172088-marker)) Note that `process.hrtime()` is only
    useful for getting relative time and can’t be used to get the current time with
    microsecond accuracy.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.html#idm46291188172088-marker)) 请注意，`process.hrtime()` 只能用于获取相对时间，不能用于获取具有微秒精度的当前时间。
- en: ^([2](ch04.html#idm46291188111672-marker)) This example doesn’t persist data
    to disk and isn’t appropriate for production use.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch04.html#idm46291188111672-marker)) 这个示例不会将数据持久化到磁盘，不适合生产使用。
